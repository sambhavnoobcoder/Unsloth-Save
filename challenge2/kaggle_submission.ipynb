{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Unsloth challenge 2- Make QLoRA work with FSDP2","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"FSDP2\"></a>\n## B) Make `QLoRA` work with `FSDP2` [Difficulty: Medium to Hard] [Max points: 10]\n\n1. Goal: Write a single Python script to finetune Llama 3.1 8B on 2x or more GPUs with FSDP2.\n\n2. You must showcase this working in a free **Kaggle notebook with 2 x Tesla T4 GPUs**.\n\n3. Pipeline parallelism is also fine, but must utilize [`zero bubble scheduling`](https://pytorch.org/docs/stable/distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble) somehow.\n\n4. Can use a pre-quantized 4bit BnB safetensor file from [Unsloth's HF page](https://huggingface.co/unsloth) or a full 16bit one, but must do QLoRA.\n\n5. Can use `accelerate` but must be FSDP2 or related - you can investigate https://github.com/huggingface/accelerate/pull/3394, Torch Titan, other repos etc.\n\n6. Must be fully `transformers` compatible - so we must use `TrainingArguments` and `Trainer`, or `TRL` related classes.\n\n7. The loss must be equivalent to single GPU training.\n\n8. You must enable all features in FSDP2 - ie showcase offloading, checkpointing, mixed precision training etc.\n\n9. You can use `nf4` from `torch AO`, but best from `bitsandbytes`.\n\n10. Finally showcase everything working in a free Kaggle 2x Tesla T4 notebook.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Parameters","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for B) Max points = 10\n```python\nif attemped_B:\n    B_score = 0\n    if FSDP2_works_with_QLoRA:\n        if torch_compile_works: B_score += 5\n        else: B_score += 3\n        if uses_part_A_and_single_kernel_and_faster: B_score += 3\n        elif uses_torchAO:\n            if torchAO_slower_than_BnB: B_score -= 3\n    elif TP_or_PP_with_QLoRA:\n        if zero_bubble: B_score += 3\n        else: B_score += 2\n    elif FSDP1_works_with_QLoRA:\n        B_score += 1\n    if kaggle_notebook_2_tesla_t4_example:\n        B_score += 2\n    else:\n        B_score = 0\n    final_score += B_score\nelse:\n    final_score -= 2\n```","metadata":{}},{"cell_type":"markdown","source":"start with library setup ","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes\n!pip install peft\n!pip install transformers\n!pip install datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}