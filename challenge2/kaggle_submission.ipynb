{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unsloth challenge 2- Make QLoRA work with FSDP2","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"FSDP2\"></a>\n## B) Make `QLoRA` work with `FSDP2` [Difficulty: Medium to Hard] [Max points: 10]\n\n1. Goal: Write a single Python script to finetune Llama 3.1 8B on 2x or more GPUs with FSDP2.\n\n2. You must showcase this working in a free **Kaggle notebook with 2 x Tesla T4 GPUs**.\n\n3. Pipeline parallelism is also fine, but must utilize [`zero bubble scheduling`](https://pytorch.org/docs/stable/distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble) somehow.\n\n4. Can use a pre-quantized 4bit BnB safetensor file from [Unsloth's HF page](https://huggingface.co/unsloth) or a full 16bit one, but must do QLoRA.\n\n5. Can use `accelerate` but must be FSDP2 or related - you can investigate https://github.com/huggingface/accelerate/pull/3394, Torch Titan, other repos etc.\n\n6. Must be fully `transformers` compatible - so we must use `TrainingArguments` and `Trainer`, or `TRL` related classes.\n\n7. The loss must be equivalent to single GPU training.\n\n8. You must enable all features in FSDP2 - ie showcase offloading, checkpointing, mixed precision training etc.\n\n9. You can use `nf4` from `torch AO`, but best from `bitsandbytes`.\n\n10. Finally showcase everything working in a free Kaggle 2x Tesla T4 notebook.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Parameters","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for B) Max points = 10\n```python\nif attemped_B:\n    B_score = 0\n    if FSDP2_works_with_QLoRA:\n        if torch_compile_works: B_score += 5\n        else: B_score += 3\n        if uses_part_A_and_single_kernel_and_faster: B_score += 3\n        elif uses_torchAO:\n            if torchAO_slower_than_BnB: B_score -= 3\n    elif TP_or_PP_with_QLoRA:\n        if zero_bubble: B_score += 3\n        else: B_score += 2\n    elif FSDP1_works_with_QLoRA:\n        B_score += 1\n    if kaggle_notebook_2_tesla_t4_example:\n        B_score += 2\n    else:\n        B_score = 0\n    final_score += B_score\nelse:\n    final_score -= 2\n```","metadata":{}},{"cell_type":"markdown","source":"start with library setup ","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes\n!pip install peft\n!pip install transformers\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T17:45:52.670484Z","iopub.execute_input":"2025-03-17T17:45:52.670810Z","iopub.status.idle":"2025-03-17T17:46:09.229462Z","shell.execute_reply.started":"2025-03-17T17:45:52.670786Z","shell.execute_reply":"2025-03-17T17:46:09.228425Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T17:47:06.032545Z","iopub.execute_input":"2025-03-17T17:47:06.032947Z","iopub.status.idle":"2025-03-17T17:47:09.270948Z","shell.execute_reply.started":"2025-03-17T17:47:06.032914Z","shell.execute_reply":"2025-03-17T17:47:09.270088Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nfrom packaging import version\n\n# Check if bitsandbytes is installed and up-to-date.\ntry:\n    import bitsandbytes\nexcept ImportError:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes\"])\n    import bitsandbytes\n\nif version.parse(bitsandbytes.__version__) < version.parse(\"0.39.0\"):\n    print(\"Updating bitsandbytes to the latest version...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"])\n    print(\"Please restart the runtime after upgrading bitsandbytes and then re-run this code.\")\n    sys.exit(0)\n\n# Set environment variables for optimal performance\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set for 2 GPUs (T4 on Kaggle)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n    \"expandable_segments:True,\"\n    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n)\n\n# Enable torch inductor for better performance (if Triton is installed)\nos.environ[\"TORCH_COMPILE_BACKEND\"] = \"inductor\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    CPUOffload,\n    ShardingStrategy,\n)\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n    apply_activation_checkpointing,\n    checkpoint_wrapper,\n    CheckpointImpl,\n)\nfrom functools import partial\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers.models.llama.modeling_llama import LlamaDecoderLayer\nfrom datasets import load_dataset\nfrom torch.cuda.amp import GradScaler\nimport time\n\n# --- Distributed Process Group Initialization for Kaggle T4 GPUs ---\ndef init_distributed():\n    \"\"\"Initialize process group for distributed training.\"\"\"\n    if \"RANK\" not in os.environ:\n        os.environ[\"RANK\"] = \"0\"\n    if \"LOCAL_RANK\" not in os.environ:\n        os.environ[\"LOCAL_RANK\"] = \"0\"\n    if \"WORLD_SIZE\" not in os.environ:\n        os.environ[\"WORLD_SIZE\"] = \"1\"\n    if \"MASTER_ADDR\" not in os.environ:\n        os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n    if \"MASTER_PORT\" not in os.environ:\n        os.environ[\"MASTER_PORT\"] = \"29500\"\n\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    rank = int(os.environ.get(\"RANK\", 0))\n\n    if not dist.is_initialized():\n        dist.init_process_group(\n            backend=\"nccl\",\n            init_method=f\"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\",\n            rank=rank,\n            world_size=world_size\n        )\n    \n    return local_rank, world_size, rank\n\nlocal_rank, world_size, rank = init_distributed()\ntorch.cuda.set_device(local_rank)\ndevice = torch.device(f\"cuda:{local_rank}\")\n\n# --- Model & Quantization Setup ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\nmax_seq_length = 1024  # Adjusted for T4 memory constraints\nbatch_size = 1  \ngradient_accumulation_steps = 4\n\n# Improved BnB config for better performance\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# Check for flash_attn availability\ntry:\n    import flash_attn  # Only needed to check if available\n    attn_impl = \"flash_attention_2\"\nexcept ImportError:\n    print(\"flash_attn package not installed. Using default attention implementation.\")\n    attn_impl = None\n\n# Prepare kwargs for model loading. Only pass attn_implementation if flash_attn is available.\nmodel_kwargs = {\n    \"device_map\": {\"\": local_rank},\n    \"quantization_config\": bnb_config,\n}\nif attn_impl is not None:\n    model_kwargs[\"attn_implementation\"] = attn_impl\n\n# Load model with BnB quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    **model_kwargs\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\n\n# Disable caching for gradient checkpointing\nmodel.config.use_cache = False\n\n# --- Apply LoRA via PEFT ---\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Optimized LoRA config with parameters that work well with FSDP\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Set parameters for gradient requirements: only LoRA parameters are trainable.\nwith torch.no_grad():\n    for name, param in model.named_parameters():\n        if \".lora_A.\" in name or \".lora_B.\" in name:\n            param.requires_grad_(True)\n        else:\n            param.requires_grad_(False)\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Configure FSDP with optimized settings ---\n# Use auto_wrap_policy to target transformer layers.\nauto_wrap_policy = partial(\n    transformer_auto_wrap_policy,\n    transformer_layer_cls={LlamaDecoderLayer},\n)\n\nmixed_precision_policy = MixedPrecision(\n    param_dtype=torch.float16,\n    reduce_dtype=torch.float16,\n    buffer_dtype=torch.float16,\n    cast_forward_inputs=True,\n)\n\ncpu_offload = CPUOffload(offload_params=True)\n\n# Build a list of modules to ignore for FSDP wrapping.\n# Here we ignore modules that contain only frozen parameters (e.g. quantized parameters).\nignored_modules = []\nfor module in model.modules():\n    params = list(module.parameters(recurse=False))\n    if params and all(not p.requires_grad for p in params):\n        ignored_modules.append(module)\n\n# Configure FSDP.\n# Note: When WORLD_SIZE is 1, FSDP may switch to NO_SHARD.\nmodel = FSDP(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    mixed_precision=mixed_precision_policy,\n    device_id=local_rank,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    limit_all_gathers=True,\n    use_orig_params=True,\n    cpu_offload=cpu_offload,\n    ignored_modules=ignored_modules,  # Exclude frozen submodules from wrapping.\n)\n\n# Apply activation checkpointing to save memory.\nnon_reentrant_wrapper = partial(\n    checkpoint_wrapper,\n    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n)\ncheck_fn = lambda submodule: isinstance(submodule, LlamaDecoderLayer)\napply_activation_checkpointing(\n    model,\n    checkpoint_wrapper_fn=non_reentrant_wrapper,\n    check_fn=check_fn,\n)\n\n# --- Training setup ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nscaler = GradScaler()  # Note: FutureWarning regarding GradScaler usage\n\n# --- TorchDynamo Config & Conditional torch.compile ---\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\n# Check if Triton is installed.\ntry:\n    import triton\n    triton_installed = True\nexcept ImportError:\n    triton_installed = False\n\nif hasattr(torch, 'compile') and triton_installed:\n    try:\n        print(\"Applying torch.compile to the model...\")\n        model = torch.compile(model, backend=\"inductor\")\n    except Exception as e:\n        print(f\"torch.compile failed: {e}. Falling back to eager mode.\")\nelse:\n    print(\"Skipping torch.compile (either not available or Triton is missing).\")\n\n# --- Dataset Preparation ---\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:5%]\")  # Smaller sample for T4 GPUs\n\ndef process(examples):\n    texts = examples[\"text\"]\n    tokenized = tokenizer(\n        texts,\n        max_length=max_seq_length,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n    return tokenized\n\ndataset = dataset.map(process, batched=True, remove_columns=dataset.column_names)\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\nif world_size > 1:\n    from torch.utils.data.distributed import DistributedSampler\n    sampler = DistributedSampler(\n        dataset,\n        num_replicas=world_size,\n        rank=rank,\n        shuffle=True,\n    )\nelse:\n    sampler = None\n\ndataloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=batch_size,\n    sampler=sampler,\n    num_workers=2,  # Reduced for T4 GPUs\n    pin_memory=True,\n)\n\n# --- Training Loop ---\nmodel.train()\ntotal_loss = 0.0\nstart_time = time.time()\n\nprint(f\"Starting training on {world_size} GPU(s) - Designed for 2x Tesla T4 on Kaggle\")\n\nfor step, batch in enumerate(dataloader):\n    if step >= 10 * gradient_accumulation_steps:\n        break\n    \n    inputs = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    \n    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=inputs)\n        loss = outputs.loss / gradient_accumulation_steps\n    \n    scaler.scale(loss).backward()\n    total_loss += loss.item()\n    \n    if (step + 1) % gradient_accumulation_steps == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        \n        if rank == 0:\n            current_time = time.time()\n            elapsed = current_time - start_time\n            steps_per_sec = (step + 1) / elapsed\n            print(f\"Step {(step+1)//gradient_accumulation_steps} | Loss: {total_loss:.4f} | Steps/sec: {steps_per_sec:.4f}\")\n        \n        total_loss = 0.0\n\nif rank == 0:\n    model.save_pretrained(\"./llama-3.1-8b-lora\")\n    print(\"Training complete. Model saved to ./llama-3.1-8b-lora\")\n\nif world_size > 1:\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T17:48:34.787464Z","iopub.execute_input":"2025-03-17T17:48:34.787812Z","iopub.status.idle":"2025-03-17T17:53:40.467579Z","shell.execute_reply.started":"2025-03-17T17:48:34.787785Z","shell.execute_reply":"2025-03-17T17:53:40.466717Z"}},"outputs":[{"name":"stdout","text":"flash_attn package not installed. Using default attention implementation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29fe9e6cdbec437999ad49d453e97f46"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78599cf20b648419a999095758bd162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351a35169ebb4726ba702eb680193e39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6927e5336a46679d75565135418a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dffb09575964178ab666994ca8d782b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74df0e2a21f8473088d33a69f958bb67"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n  warnings.warn(\n<ipython-input-3-a0abe9812998>:203: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # Note: FutureWarning regarding GradScaler usage\n","output_type":"stream"},{"name":"stdout","text":"Skipping torch.compile (either not available or Triton is missing).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"unified_chip2.jsonl:   0%|          | 0.00/95.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d1e77b9cf604901abfcd3d803091cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8161c4757db4e27a7e1a4a3c73f426c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10514 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1a1f7bf3aa48e385e05a61347eb008"}},"metadata":{}},{"name":"stdout","text":"Starting training on 1 GPU(s) - Designed for 2x Tesla T4 on Kaggle\nStep 1 | Loss: 15.3889 | Steps/sec: 0.1646\nStep 2 | Loss: 15.5869 | Steps/sec: 0.1751\nStep 3 | Loss: 14.5241 | Steps/sec: 0.1724\nStep 4 | Loss: 13.4575 | Steps/sec: 0.1701\nStep 5 | Loss: 12.0819 | Steps/sec: 0.1654\nStep 6 | Loss: 10.8009 | Steps/sec: 0.1630\nStep 7 | Loss: 10.5190 | Steps/sec: 0.1620\nStep 8 | Loss: 9.4808 | Steps/sec: 0.1607\nStep 9 | Loss: 9.1467 | Steps/sec: 0.1596\nStep 10 | Loss: 8.7568 | Steps/sec: 0.1589\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:768: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training complete. Model saved to ./llama-3.1-8b-lora\n","output_type":"stream"}],"execution_count":3}]}