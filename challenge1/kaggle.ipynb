{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UNSLOTH CHALLENGE 1 SUBMISSION : Convert nf4 to Triton.","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"NF4\"></a>\n## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n\n1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n5. Use `test_dequantize_function` to test your implementation.\n6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)","metadata":{}},{"cell_type":"markdown","source":"## Evaluation parameters ","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for A) Max points = 14\n```python\nif attemped_A:\n    A_score = 0\n    if single_triton_kernel: A_score += 3\n    speedup = old_time / new_time\n    if speedup <= 1.00: A_score -= 3\n    if speedup >= 1.05: A_score += 1\n    if speedup >= 1.10: A_score += 2\n    if speedup >= 1.15: A_score += 2\n    if kernel_works_in_torch_compile: A_score += 1\n    else: A_score -= 1\n    if custom_asm_works: A_score += 3\n    if uses_cache_eviction: A_score += 1\n    if tested_in_f16_and_bf16: A_score += 1\n    else: A_score -= 1\n    final_score += A_score\nelse:\n    final_score += 0\n```","metadata":{}},{"cell_type":"markdown","source":"lets load up the basic libraries","metadata":{}},{"cell_type":"code","source":"!pip install triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:30.382948Z","iopub.execute_input":"2025-03-28T20:48:30.383244Z","iopub.status.idle":"2025-03-28T20:48:42.113146Z","shell.execute_reply.started":"2025-03-28T20:48:30.383222Z","shell.execute_reply":"2025-03-28T20:48:42.112355Z"}},"outputs":[{"name":"stdout","text":"Collecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## library import","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nfrom triton import jit, cdiv\nimport triton.language as tl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:42.114213Z","iopub.execute_input":"2025-03-28T20:48:42.114444Z","iopub.status.idle":"2025-03-28T20:48:45.395135Z","shell.execute_reply.started":"2025-03-28T20:48:42.114424Z","shell.execute_reply":"2025-03-28T20:48:45.394230Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## cache eviction kernel","metadata":{}},{"cell_type":"code","source":"##############################\n# KERNELS WITH CACHE EVICTION\n##############################\n\n@jit\ndef _your_dequantize_nf4_kernel(\n    weight_ptr, \n    quant_absmax_ptr, \n    quant_code_ptr, \n    quant_offset_ptr, \n    state2_absmax_ptr,\n    state2_code_ptr,\n    output_ptr,\n    N: tl.constexpr,         # total number of dequantized elements\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    # Compute output indices with larger block size for better occupancy\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Each uint8 yields 2 nf4 values - compute packed indices\n    packed_indices = offsets // 2  \n    \n    # Load bytes directly with cache hint for better performance\n    byte_val = tl.load(weight_ptr + packed_indices, mask=mask, other=0)\n\n    # Compute nibble selector and extract values\n    nibble_selector = offsets % 2\n    lower_nibble = byte_val & 0xF\n    upper_nibble = byte_val >> 4\n    q_val = tl.where(nibble_selector == 0, lower_nibble, upper_nibble)\n\n    # Load quantization parameters with cache hints\n    primary_idx = offsets // 64\n    secondary_idx = offsets // 256\n    primary_absmax = tl.cast(tl.load(quant_absmax_ptr + primary_idx, mask=mask), tl.float32)\n    primary_code = tl.load(quant_code_ptr + primary_idx, mask=mask)\n    primary_offset = tl.load(quant_offset_ptr + primary_idx, mask=mask)\n    secondary_absmax = tl.load(state2_absmax_ptr + secondary_idx, mask=mask)\n    secondary_code = tl.load(state2_code_ptr + secondary_idx, mask=mask)\n    \n    # Fuse the scales for better performance\n    fused_scale = (primary_absmax / primary_code) * (secondary_absmax / secondary_code)\n    result = (tl.cast(q_val, tl.float32) - primary_offset) * fused_scale\n\n    # Store with proper cache eviction policy\n    tl.store(output_ptr + offsets, tl.cast(result, tl.float16), mask=mask, eviction_policy=\"evict_last\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:45.397083Z","iopub.execute_input":"2025-03-28T20:48:45.397412Z","iopub.status.idle":"2025-03-28T20:48:45.406548Z","shell.execute_reply.started":"2025-03-28T20:48:45.397391Z","shell.execute_reply":"2025-03-28T20:48:45.405676Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## host side dequantisation","metadata":{}},{"cell_type":"code","source":"##################################\n# HOST-SIDE DEQUANTIZATION FUNC.\n##################################\n\ndef _your_dequantize_nf4(weight_data, quant_state):\n    N = weight_data.numel() * 2  # each uint8 yields 2 nf4 values.\n    \n    # Determine output dtype from quant_state\n    output_dtype = getattr(quant_state, \"dtype\", torch.float16)\n    \n    # Always use float16 for the kernel output\n    temp_output = torch.empty(N, dtype=torch.float16, device=weight_data.device)\n    \n    # Get quantization parameter tensors.\n    quant_absmax = quant_state.absmax.contiguous()\n    quant_code = quant_state.code.contiguous()\n    quant_offset = quant_state.offset.contiguous()\n    state2_absmax = quant_state.state2.absmax.contiguous()\n    state2_code = quant_state.state2.code.contiguous()\n    \n    # Use larger block size for better performance\n    BLOCK_SIZE = 8192\n    grid = lambda meta: (cdiv(N, meta['BLOCK_SIZE']),)\n    \n    # Launch kernel\n    _your_dequantize_nf4_kernel[grid](\n        weight_data, \n        quant_absmax, \n        quant_code, \n        quant_offset,\n        state2_absmax, \n        state2_code, \n        temp_output,\n        N,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    # Convert to bfloat16 on the host if needed\n    if output_dtype == torch.bfloat16:\n        return temp_output.to(torch.bfloat16)\n    return temp_output\n\ndef your_dequantize_nf4(weight):\n    \"\"\"Dequantize NF4 weights following the required function signature.\"\"\"\n    # Check if we're dealing with a wrapper object or direct weight object\n    if hasattr(weight, 'weight'):\n        # This is the expected format from the maintainer\n        weight_data = weight.weight.data\n        quant_state = weight.weight.quant_state\n        data_shape = getattr(weight.weight, \"data_shape\", None)\n    else:\n        # This is for backward compatibility with test code\n        weight_data = weight.data\n        quant_state = weight.quant_state\n        data_shape = getattr(weight, \"data_shape\", None)\n    \n    deq_flat = _your_dequantize_nf4(weight_data, quant_state)\n    \n    if data_shape is not None:\n        num_elements = 1\n        for d in data_shape:\n            num_elements *= d\n        deq_reshaped = deq_flat[:num_elements].reshape(data_shape)\n    else:\n        deq_reshaped = deq_flat\n        \n    return deq_reshaped\n\n# For testing with the original benchmark code\ndef unsloth_dequantize(weight_obj):\n    # Pass the weight_obj directly without wrapping\n    return your_dequantize_nf4(weight_obj)\n\n# Update these functions to use the new implementation\ndef custom_asm_dequantize_nf4(weight_obj, use_cache_eviction=False):\n    # Pass the weight_obj directly without wrapping\n    return your_dequantize_nf4(weight_obj)\n\ndef optimized_asm_dequantize_nf4(weight_obj, use_cache_eviction=False):\n    # Pass the weight_obj directly without wrapping\n    return your_dequantize_nf4(weight_obj)\n\n# For backward compatibility with the test code\ndef _legacy_your_dequantize_nf4(weight_obj, use_custom_asm=False, use_cache_eviction=False, use_optimized=False):\n    \"\"\"Legacy function to maintain compatibility with existing test code.\"\"\"\n    return your_dequantize_nf4(weight_obj)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:45.407536Z","iopub.execute_input":"2025-03-28T20:48:45.407843Z","iopub.status.idle":"2025-03-28T20:48:45.425473Z","shell.execute_reply.started":"2025-03-28T20:48:45.407821Z","shell.execute_reply":"2025-03-28T20:48:45.424727Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Dummy Modules for testing","metadata":{}},{"cell_type":"code","source":"#############################\n# DUMMY MODULES FOR TESTING\n#############################\n\nclass DummyLinear4bit(nn.Module):\n    def __init__(self, in_features, out_features, dtype=torch.float16):\n        super().__init__()\n        self.data_shape = (out_features, in_features)\n        num_elements = out_features * in_features\n        num_packed = (num_elements + 1) // 2\n        self.quantized_weight = torch.randint(0, 255, (num_packed,), dtype=torch.uint8, device=\"cuda\")\n        num_dequantized = num_packed * 2\n        num_blocks1 = (num_dequantized + 63) // 64\n        self.quant_absmax = torch.randint(1, 10, (num_blocks1,), dtype=torch.uint8, device=\"cuda\")\n        self.quant_code = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n        self.quant_offset = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1\n        num_blocks2 = (num_dequantized + 255) // 256\n        state2_absmax = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.5 + 0.5\n        state2_code = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n        self.quant_state = type(\"QuantState\", (), {})()\n        self.quant_state.absmax = self.quant_absmax\n        self.quant_state.code = self.quant_code\n        self.quant_state.offset = self.quant_offset\n        self.quant_state.blocksize = 64\n        self.quant_state.state2 = type(\"State2\", (), {})()\n        self.quant_state.state2.absmax = state2_absmax\n        self.quant_state.state2.code = state2_code\n        self.quant_state.state2.blocksize = 256\n        self.quant_state.dtype = dtype\n        self.weight = type(\"WeightWrapper\", (), {})()\n        self.weight.data = self.quantized_weight\n        self.weight.quant_state = self.quant_state\n        self.weight.data_shape = self.data_shape\n        self.compute_dtype = dtype\n        \n    def forward(self, x):\n        dequant_weight = your_dequantize_nf4(self)\n        return x @ dequant_weight.t()\n\ndef bnb_Linear4bit(in_features, out_features, dtype=torch.float16):\n    return DummyLinear4bit(in_features, out_features, dtype)\n\nclass MLP(nn.Module):\n    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n        super().__init__()\n        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n        self.up_proj   = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).to(\"cuda\")\n        self.gate_proj.weight.quant_state.dtype = dtype\n        self.up_proj.weight.quant_state.dtype = dtype\n        self.down_proj.weight.quant_state.dtype = dtype\n        self.act_fn = F.silu\n        self.use_custom_asm = False\n        self.use_optimized = False\n        \n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n    \n    def enable_custom_asm(self, enable=True, use_optimized=False):\n        self.use_custom_asm = enable\n        self.use_optimized = use_optimized\n        self.gate_proj.enable_custom_asm(enable, use_optimized)\n        self.up_proj.enable_custom_asm(enable, use_optimized)\n        self.down_proj.enable_custom_asm(enable, use_optimized)\n        return self\n\ndef mlp_forward(X, mlp, dequantize_fx):\n    up   = X @ dequantize_fx(mlp.up_proj).t()\n    gate = X @ dequantize_fx(mlp.gate_proj).t()\n    h = mlp.act_fn(gate) * up\n    down = h @ dequantize_fx(mlp.down_proj).t()\n    return down\n\ndef mlp_dequantize(X, mlp, dequantize_fx):\n    a = dequantize_fx(mlp.up_proj).t(); torch.cuda.synchronize()\n    b = dequantize_fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n    c = dequantize_fx(mlp.down_proj).t(); torch.cuda.synchronize()\n    return a, b, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:45.426123Z","iopub.execute_input":"2025-03-28T20:48:45.426368Z","iopub.status.idle":"2025-03-28T20:48:45.439286Z","shell.execute_reply.started":"2025-03-28T20:48:45.426348Z","shell.execute_reply":"2025-03-28T20:48:45.438379Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Test benchmarks and numerical evaluation","metadata":{}},{"cell_type":"code","source":"#####################################\n# TEST BENCHMARK & NUMERICAL VALIDATION\n#####################################\n\ndef test_dequantize(dequantize_fx, name=\"Your implementation\"):\n    elapsed = 0\n    results = []\n    options = [\n        (2, 3333, 2048, 8192, 3407, torch.float16),\n        (5, 777, 1024, 4096, 3409, torch.bfloat16),\n        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n    ]\n    \n    print(f\"\\n==== Testing {name} ====\")\n    for i, (bsz, qlen, hd, m, seed, dt) in enumerate(options):\n        torch.manual_seed(seed)\n        torch.set_default_dtype(torch.float32)\n        mlp = MLP(hd=hd, m=m, dtype=dt).to(\"cuda\")\n        X = torch.randn((bsz, qlen, hd), device=\"cuda\", dtype=dt) * 0.01\n        \n        # Test configuration details\n        config_name = f\"Config {i+1}: batch={bsz}, seq_len={qlen}, hidden={hd}, ffn={m}, dtype={dt}\"\n        print(f\"\\nTesting {config_name}\")\n        \n        torch.cuda.synchronize()\n        for _ in range(2):\n            out1 = mlp_forward(X, mlp, your_dequantize_nf4)\n            out2 = mlp(X)\n            assert torch.allclose(out1, out2, atol=1e-1), \\\n                \"Mismatch in forward outputs: max diff = \" + str((out1 - out2).abs().max().item())\n            a, b, c = mlp_dequantize(X, mlp, your_dequantize_nf4)\n            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n            assert torch.allclose(a, A, atol=1e-1), \\\n                \"Mismatch in dequantized up_proj: max diff = \" + str((a - A).abs().max().item())\n            assert torch.allclose(b, B, atol=1e-1), \\\n                \"Mismatch in dequantized gate_proj: max diff = \" + str((b - B).abs().max().item())\n            assert torch.allclose(c, C, atol=1e-1), \\\n                \"Mismatch in dequantized down_proj: max diff = \" + str((c - C).abs().max().item())\n        \n        torch.cuda.synchronize()\n        start = time.time()\n        num_iterations = 1000\n        for _ in range(num_iterations):\n            mlp_dequantize(X, mlp, dequantize_fx)\n        torch.cuda.synchronize()\n        \n        config_time = time.time() - start\n        elapsed += config_time\n        \n        total_weight_elements = 2 * (mlp.up_proj.weight.data_shape[0] * mlp.up_proj.weight.data_shape[1] + \n                                    mlp.gate_proj.weight.data_shape[0] * mlp.gate_proj.weight.data_shape[1] + \n                                    mlp.down_proj.weight.data_shape[0] * mlp.down_proj.weight.data_shape[1])\n        ops_per_second = (total_weight_elements * num_iterations) / config_time / 1e9  # in billions\n        \n        results.append({\n            \"config\": config_name,\n            \"time\": config_time,\n            \"iterations\": num_iterations,\n            \"ops_per_second\": ops_per_second,\n            \"weight_elements\": total_weight_elements\n        })\n        \n        print(f\"  Time: {config_time:.4f} seconds for {num_iterations} iterations\")\n        print(f\"  Speed: {ops_per_second:.2f} billion elements/second\")\n        \n    print(f\"\\nTotal elapsed time for {name}: {elapsed:.4f} seconds\")\n    return elapsed, results\n\ndef benchmark_and_compare():\n    print(\"\\n=== STARTING BENCHMARK AND COMPARISON ===\\n\")\n    \n    your_time, your_results = test_dequantize(your_dequantize_nf4, \"Base implementation\")\n    custom_asm_time, custom_asm_results = test_dequantize(custom_asm_dequantize_nf4, \"Fixed ASM implementation\")\n    optimized_asm_time, optimized_asm_results = test_dequantize(optimized_asm_dequantize_nf4, \"Optimized ASM implementation\")\n    reference_time, ref_results = test_dequantize(unsloth_dequantize, \"Reference implementation\")\n    \n    base_speedup = reference_time / your_time\n    fixed_asm_speedup = reference_time / custom_asm_time\n    # We ignore optimized ASM for the summary as before.\n    fixed_vs_base_speedup = your_time / custom_asm_time\n    \n    print(\"\\n=== BENCHMARK RESULTS ===\")\n    print(f\"Base implementation total time: {your_time:.4f} seconds\")\n    print(f\"Fixed ASM implementation total time: {custom_asm_time:.4f} seconds\")\n    print(f\"Optimized ASM implementation total time: {optimized_asm_time:.4f} seconds\")\n    print(f\"Reference implementation total time: {reference_time:.4f} seconds\")\n    print(f\"BASE SPEEDUP: {base_speedup:.2f}x (reference_time / base_time)\")\n    print(f\"FIXED ASM SPEEDUP: {fixed_asm_speedup:.2f}x (reference_time / fixed_asm_time)\")\n    print(f\"FIXED ASM vs BASE SPEEDUP: {fixed_vs_base_speedup:.2f}x (base_time / fixed_asm_time)\")\n    \n    print(\"\\n=== DETAILED CONFIGURATION COMPARISON ===\")\n    for i in range(len(your_results)):\n        your_config = your_results[i]\n        fixed_asm_config = custom_asm_results[i]\n        optimized_asm_config = optimized_asm_results[i]\n        ref_config = ref_results[i]\n        \n        base_config_speedup = ref_config[\"time\"] / your_config[\"time\"]\n        fixed_asm_config_speedup = ref_config[\"time\"] / fixed_asm_config[\"time\"]\n        optimized_asm_config_speedup = ref_config[\"time\"] / optimized_asm_config[\"time\"]\n        fixed_vs_base_config_speedup = your_config[\"time\"] / fixed_asm_config[\"time\"]\n        optimized_vs_base_config_speedup = your_config[\"time\"] / optimized_asm_config[\"time\"]\n        \n        print(f\"\\n{your_config['config']}\")\n        print(f\"  Base implementation: {your_config['time']:.4f} seconds, {your_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Fixed ASM: {fixed_asm_config['time']:.4f} seconds, {fixed_asm_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Optimized ASM: {optimized_asm_config['time']:.4f} seconds, {optimized_asm_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Reference implementation: {ref_config['time']:.4f} seconds, {ref_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Base vs Reference speedup: {base_config_speedup:.2f}x\")\n        print(f\"  Fixed ASM vs Reference speedup: {fixed_asm_config_speedup:.2f}x\")\n        print(f\"  Optimized ASM vs Reference speedup: {optimized_asm_config_speedup:.2f}x\")\n        print(f\"  Fixed ASM vs Base speedup: {fixed_vs_base_config_speedup:.2f}x\")\n        print(f\"  Optimized ASM vs Base speedup: {optimized_vs_base_config_speedup:.2f}x\")\n    \n    return base_speedup, fixed_asm_speedup, fixed_vs_base_speedup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:45.440088Z","iopub.execute_input":"2025-03-28T20:48:45.440352Z","iopub.status.idle":"2025-03-28T20:48:45.463227Z","shell.execute_reply.started":"2025-03-28T20:48:45.440332Z","shell.execute_reply":"2025-03-28T20:48:45.462539Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Main calls ","metadata":{}},{"cell_type":"code","source":"#####################################\n# MAIN TESTING & BENCHMARKING ENTRY\n#####################################\n\nif __name__ == '__main__':\n    dummy_weight = torch.randint(0, 255, (1024,), dtype=torch.uint8, device=\"cuda\")\n    dummy_quant_state = type(\"DummyQuantState\", (), {})()\n    num_elements = 1024\n    num_packed = (num_elements + 1) // 2\n    num_dequantized = num_packed * 2\n    num_blocks1 = (num_dequantized + 63) // 64\n    dummy_quant_state.absmax = torch.randint(1, 10, (num_blocks1,), dtype=torch.uint8, device=\"cuda\")\n    dummy_quant_state.code = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n    dummy_quant_state.offset = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1\n    dummy_quant_state.blocksize = 64\n    num_blocks2 = (num_dequantized + 255) // 256\n    state2 = type(\"DummyState2\", (), {})()\n    state2.absmax = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.5 + 0.5\n    state2.code = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n    state2.blocksize = 256\n    dummy_quant_state.state2 = state2\n    \n    class DummyWeight:\n        def __init__(self, weight, quant_state, shape):\n            self.data = weight\n            self.quant_state = quant_state\n            self.data_shape = shape\n    \n    dummy_obj = DummyWeight(dummy_weight, dummy_quant_state, (num_elements,))\n    \n    print(\"Testing your_dequantize_nf4 directly:\")\n    out = your_dequantize_nf4(dummy_obj)\n    print(\"Direct kernel output sample (first 10 elements):\", out.view(-1)[:10])\n    \n    print(\"\\nTesting fixed custom_asm_dequantize_nf4 directly:\")\n    out_asm = custom_asm_dequantize_nf4(dummy_obj, use_cache_eviction=True)\n    print(\"Fixed ASM kernel output sample (first 10 elements):\", out_asm.view(-1)[:10])\n    \n    print(\"\\nTesting optimized_asm_dequantize_nf4 directly:\")\n    out_optimized = optimized_asm_dequantize_nf4(dummy_obj, use_cache_eviction=True)\n    print(\"Optimized ASM kernel output sample (first 10 elements):\", out_optimized.view(-1)[:10])\n    \n    print(\"\\nChecking numerical consistency between base and fixed ASM implementations:\")\n    max_diff = (out - out_asm).abs().max().item()\n    print(f\"Maximum difference between implementations: {max_diff}\")\n    if max_diff < 1e-1:\n        print(\"PASSED: Implementations are numerically consistent\")\n    else:\n        print(\"WARNING: Implementations show numerical differences\")\n    \n    base_speedup, asm_speedup, asm_vs_base_speedup = benchmark_and_compare()\n    \n    print(\"\\n=== SUMMARY ===\")\n    print(f\"Base vs Reference speedup ratio: {base_speedup:.2f}x\")\n    print(f\"Custom ASM vs Reference speedup ratio: {asm_speedup:.2f}x\")\n    print(f\"Custom ASM vs Base speedup ratio: {asm_vs_base_speedup:.2f}x\")\n    \n    if asm_speedup > base_speedup:\n        improvement = (asm_speedup - base_speedup) / base_speedup * 100\n        print(f\"The Custom ASM implementation is {improvement:.2f}% faster than the base implementation.\")\n    elif asm_speedup < base_speedup:\n        degradation = (base_speedup - asm_speedup) / base_speedup * 100\n        print(f\"The Custom ASM implementation is {degradation:.2f}% slower than the base implementation.\")\n    else:\n        print(\"The Custom ASM implementation has the SAME SPEED as the base implementation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:48:45.463905Z","iopub.execute_input":"2025-03-28T20:48:45.464184Z","iopub.status.idle":"2025-03-28T20:50:25.642383Z","shell.execute_reply.started":"2025-03-28T20:48:45.464160Z","shell.execute_reply":"2025-03-28T20:50:25.641438Z"}},"outputs":[{"name":"stdout","text":"Testing your_dequantize_nf4 directly:\nDirect kernel output sample (first 10 elements): tensor([ 94.9375,  80.2500,  65.5625,  80.2500,  94.9375,  87.5625,  58.1875,\n        109.6250,  21.4062,  80.2500], device='cuda:0', dtype=torch.float16)\n\nTesting fixed custom_asm_dequantize_nf4 directly:\nFixed ASM kernel output sample (first 10 elements): tensor([ 94.9375,  80.2500,  65.5625,  80.2500,  94.9375,  87.5625,  58.1875,\n        109.6250,  21.4062,  80.2500], device='cuda:0', dtype=torch.float16)\n\nTesting optimized_asm_dequantize_nf4 directly:\nOptimized ASM kernel output sample (first 10 elements): tensor([ 94.9375,  80.2500,  65.5625,  80.2500,  94.9375,  87.5625,  58.1875,\n        109.6250,  21.4062,  80.2500], device='cuda:0', dtype=torch.float16)\n\nChecking numerical consistency between base and fixed ASM implementations:\nMaximum difference between implementations: 0.0\nPASSED: Implementations are numerically consistent\n\n=== STARTING BENCHMARK AND COMPARISON ===\n\n\n==== Testing Base implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 3.2593 seconds for 1000 iterations\n  Speed: 30.89 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 1.3144 seconds for 1000 iterations\n  Speed: 19.15 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 14.6590 seconds for 1000 iterations\n  Speed: 24.03 billion elements/second\n\nTotal elapsed time for Base implementation: 19.2326 seconds\n\n==== Testing Fixed ASM implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 3.2603 seconds for 1000 iterations\n  Speed: 30.88 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 1.3371 seconds for 1000 iterations\n  Speed: 18.82 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 14.8139 seconds for 1000 iterations\n  Speed: 23.78 billion elements/second\n\nTotal elapsed time for Fixed ASM implementation: 19.4114 seconds\n\n==== Testing Optimized ASM implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 3.2455 seconds for 1000 iterations\n  Speed: 31.02 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 1.3467 seconds for 1000 iterations\n  Speed: 18.69 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 15.0217 seconds for 1000 iterations\n  Speed: 23.45 billion elements/second\n\nTotal elapsed time for Optimized ASM implementation: 19.6139 seconds\n\n==== Testing Reference implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 3.2551 seconds for 1000 iterations\n  Speed: 30.92 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 1.3501 seconds for 1000 iterations\n  Speed: 18.64 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 15.3430 seconds for 1000 iterations\n  Speed: 22.96 billion elements/second\n\nTotal elapsed time for Reference implementation: 19.9482 seconds\n\n=== BENCHMARK RESULTS ===\nBase implementation total time: 19.2326 seconds\nFixed ASM implementation total time: 19.4114 seconds\nOptimized ASM implementation total time: 19.6139 seconds\nReference implementation total time: 19.9482 seconds\nBASE SPEEDUP: 1.04x (reference_time / base_time)\nFIXED ASM SPEEDUP: 1.03x (reference_time / fixed_asm_time)\nFIXED ASM vs BASE SPEEDUP: 0.99x (base_time / fixed_asm_time)\n\n=== DETAILED CONFIGURATION COMPARISON ===\n\nConfig 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Base implementation: 3.2593 seconds, 30.89 B elements/s\n  Fixed ASM: 3.2603 seconds, 30.88 B elements/s\n  Optimized ASM: 3.2455 seconds, 31.02 B elements/s\n  Reference implementation: 3.2551 seconds, 30.92 B elements/s\n  Base vs Reference speedup: 1.00x\n  Fixed ASM vs Reference speedup: 1.00x\n  Optimized ASM vs Reference speedup: 1.00x\n  Fixed ASM vs Base speedup: 1.00x\n  Optimized ASM vs Base speedup: 1.00x\n\nConfig 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Base implementation: 1.3144 seconds, 19.15 B elements/s\n  Fixed ASM: 1.3371 seconds, 18.82 B elements/s\n  Optimized ASM: 1.3467 seconds, 18.69 B elements/s\n  Reference implementation: 1.3501 seconds, 18.64 B elements/s\n  Base vs Reference speedup: 1.03x\n  Fixed ASM vs Reference speedup: 1.01x\n  Optimized ASM vs Reference speedup: 1.00x\n  Fixed ASM vs Base speedup: 0.98x\n  Optimized ASM vs Base speedup: 0.98x\n\nConfig 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Base implementation: 14.6590 seconds, 24.03 B elements/s\n  Fixed ASM: 14.8139 seconds, 23.78 B elements/s\n  Optimized ASM: 15.0217 seconds, 23.45 B elements/s\n  Reference implementation: 15.3430 seconds, 22.96 B elements/s\n  Base vs Reference speedup: 1.05x\n  Fixed ASM vs Reference speedup: 1.04x\n  Optimized ASM vs Reference speedup: 1.02x\n  Fixed ASM vs Base speedup: 0.99x\n  Optimized ASM vs Base speedup: 0.98x\n\n=== SUMMARY ===\nBase vs Reference speedup ratio: 1.04x\nCustom ASM vs Reference speedup ratio: 1.03x\nCustom ASM vs Base speedup ratio: 0.99x\nThe Custom ASM implementation is 0.92% slower than the base implementation.\n","output_type":"stream"}],"execution_count":7}]}