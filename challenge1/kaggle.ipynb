{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UNSLOTH CHALLENGE 1 SUBMISSION : Convert nf4 to Triton.","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"NF4\"></a>\n## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n\n1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n5. Use `test_dequantize_function` to test your implementation.\n6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)","metadata":{}},{"cell_type":"markdown","source":"## Evaluation parameters ","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for A) Max points = 14\n```python\nif attemped_A:\n    A_score = 0\n    if single_triton_kernel: A_score += 3\n    speedup = old_time / new_time\n    if speedup <= 1.00: A_score -= 3\n    if speedup >= 1.05: A_score += 1\n    if speedup >= 1.10: A_score += 2\n    if speedup >= 1.15: A_score += 2\n    if kernel_works_in_torch_compile: A_score += 1\n    else: A_score -= 1\n    if custom_asm_works: A_score += 3\n    if uses_cache_eviction: A_score += 1\n    if tested_in_f16_and_bf16: A_score += 1\n    else: A_score -= 1\n    final_score += A_score\nelse:\n    final_score += 0\n```","metadata":{}},{"cell_type":"markdown","source":"lets load up the basic libraries","metadata":{}},{"cell_type":"code","source":"!pip install triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:32:48.551143Z","iopub.execute_input":"2025-03-17T14:32:48.551448Z","iopub.status.idle":"2025-03-17T14:33:01.735653Z","shell.execute_reply.started":"2025-03-17T14:32:48.551419Z","shell.execute_reply":"2025-03-17T14:33:01.734596Z"}},"outputs":[{"name":"stdout","text":"Collecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"phase one - get it to compile","metadata":{}},{"cell_type":"code","source":"import torch\nfrom triton import jit, cdiv\nimport triton.language as tl\n\n# Kernel: Traverses the input tensor in blocks and copies each element after a simple cast.\n@jit\ndef _your_dequantize_nf4_kernel(weight_ptr, quant_state_ptr, output_ptr, num_elements: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    # Get the unique program (block) ID.\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    # Create a vector of indices for the block.\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to avoid out-of-bound accesses.\n    mask = offsets < num_elements\n\n    # Load values from the input weight tensor.\n    # (Note: Currently, we ignore quant_state in the arithmetic.)\n    values = tl.load(weight_ptr + offsets, mask=mask)\n    # For now, simply cast the values to float16 and store in output.\n    tl.store(output_ptr + offsets, tl.cast(values, tl.float16), mask=mask)\n\ndef _your_dequantize_nf4(weight, quant_state):\n    # Debug prints on the host side\n    print(\"=== Starting _your_dequantize_nf4 ===\")\n    print(\"Weight tensor shape:\", weight.shape)\n    print(\"Weight tensor dtype:\", weight.dtype)\n    try:\n        print(\"Quant state shape:\", quant_state.shape)\n    except AttributeError:\n        print(\"Quant state does not have a shape attribute (likely not a tensor).\")\n    \n    # Total number of elements in the weight tensor.\n    num_elements = weight.numel()\n    print(\"Total number of elements:\", num_elements)\n    \n    # Allocate an output tensor on the same device, with fp16 precision.\n    output = torch.empty(num_elements, dtype=torch.float16, device=weight.device)\n    \n    # Determine grid size based on BLOCK_SIZE.\n    grid = lambda meta: (cdiv(num_elements, meta['BLOCK_SIZE']),)\n    \n    # Launch the kernel.\n    _your_dequantize_nf4_kernel[grid](weight, quant_state, output, num_elements, BLOCK_SIZE=1024)\n    \n    # Synchronize and print a small sample of the output for debugging.\n    torch.cuda.synchronize()\n    print(\"Kernel execution complete. Output sample (first 10 elements):\", output[:10])\n    print(\"=== Finished _your_dequantize_nf4 ===\")\n    return output\n\ndef your_dequantize_nf4(weight):\n    # weight is expected to be an object with attributes 'weight.data' and 'weight.quant_state'\n    print(\">>> Entering your_dequantize_nf4\")\n    output = _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)\n    print(\">>> Exiting your_dequantize_nf4\")\n    return output\n\n# For debugging purposes, you can create dummy inputs as follows:\nif __name__ == '__main__':\n    # Create a dummy quantized weight tensor (simulate with uint8 data).\n    dummy_weight = torch.randint(0, 255, (1024,), dtype=torch.uint8, device=\"cuda\")\n    # Create a dummy quant_state tensor (its content is not used in this phase).\n    dummy_quant_state = torch.empty((1,), dtype=torch.uint8, device=\"cuda\")\n    \n    # Wrap dummy_weight in an object with the expected attributes.\n    class DummyWeight:\n        def __init__(self, weight, quant_state):\n            self.weight = type(\"W\", (), {\"data\": weight, \"quant_state\": quant_state})\n    \n    dummy_obj = DummyWeight(dummy_weight, dummy_quant_state)\n    \n    # Run our dequantization kernel skeleton.\n    output = your_dequantize_nf4(dummy_obj)\n    print(\"Final output (first 10 elements):\", output[:10])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}