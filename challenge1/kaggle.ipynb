{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UNSLOTH CHALLENGE 1 SUBMISSION : Convert nf4 to Triton.","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"NF4\"></a>\n## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n\n1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n5. Use `test_dequantize_function` to test your implementation.\n6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)","metadata":{}},{"cell_type":"markdown","source":"## Evaluation parameters ","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for A) Max points = 14\n```python\nif attemped_A:\n    A_score = 0\n    if single_triton_kernel: A_score += 3\n    speedup = old_time / new_time\n    if speedup <= 1.00: A_score -= 3\n    if speedup >= 1.05: A_score += 1\n    if speedup >= 1.10: A_score += 2\n    if speedup >= 1.15: A_score += 2\n    if kernel_works_in_torch_compile: A_score += 1\n    else: A_score -= 1\n    if custom_asm_works: A_score += 3\n    if uses_cache_eviction: A_score += 1\n    if tested_in_f16_and_bf16: A_score += 1\n    else: A_score -= 1\n    final_score += A_score\nelse:\n    final_score += 0\n```","metadata":{}},{"cell_type":"markdown","source":"lets load up the basic libraries","metadata":{}},{"cell_type":"code","source":"!pip install triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:32:48.551143Z","iopub.execute_input":"2025-03-17T14:32:48.551448Z","iopub.status.idle":"2025-03-17T14:33:01.735653Z","shell.execute_reply.started":"2025-03-17T14:32:48.551419Z","shell.execute_reply":"2025-03-17T14:33:01.734596Z"}},"outputs":[{"name":"stdout","text":"Collecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## library import","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nfrom triton import jit, cdiv\nimport triton.language as tl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T15:13:02.492214Z","iopub.execute_input":"2025-03-17T15:13:02.492527Z","iopub.status.idle":"2025-03-17T15:13:02.496269Z","shell.execute_reply.started":"2025-03-17T15:13:02.492499Z","shell.execute_reply":"2025-03-17T15:13:02.495562Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## cache eviction kernel","metadata":{}},{"cell_type":"code","source":"##############################\n# KERNELS WITH CACHE EVICTION\n##############################\n\n@jit\ndef _your_dequantize_nf4_kernel_vectorized(\n    weight_ptr, \n    quant_absmax_ptr, \n    quant_code_ptr, \n    quant_offset_ptr, \n    state2_absmax_ptr,\n    state2_code_ptr,\n    output_ptr,\n    evict_ptr,               # new pointer for cache eviction\n    N: tl.constexpr,         # total number of dequantized elements\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    # Compute output indices.\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Dummy cache eviction load.\n    _ = tl.load(evict_ptr + offsets, mask=mask, other=0)\n\n    # Each uint8 yields 2 nf4 values.\n    packed_indices = offsets // 2  \n    # Group 4 uint8 values together.\n    vec_size = 4\n    vec_indices = packed_indices // vec_size  # index in the uint32 view.\n    rem = packed_indices % vec_size           # which byte in the 32-bit word.\n\n    # Load 32 bits (i.e. 4 uint8) at once.\n    vec_data = tl.load(weight_ptr + vec_indices, mask=mask, other=0)\n    # Extract the desired byte.\n    byte_val = (vec_data >> (rem * 8)) & 0xFF\n\n    # Compute nibble selector: 0 for lower nibble, 1 for upper nibble.\n    nibble_selector = offsets % 2\n    lower_nibble = byte_val & 0xF\n    upper_nibble = byte_val >> 4\n    q_val = tl.where(nibble_selector == 0, lower_nibble, upper_nibble)\n\n    # Load quantization parameters.\n    primary_idx = offsets // 64\n    secondary_idx = offsets // 256\n    primary_absmax = tl.cast(tl.load(quant_absmax_ptr + primary_idx, mask=mask), tl.float32)\n    primary_code = tl.load(quant_code_ptr + primary_idx, mask=mask)\n    primary_offset = tl.load(quant_offset_ptr + primary_idx, mask=mask)\n    secondary_absmax = tl.load(state2_absmax_ptr + secondary_idx, mask=mask)\n    secondary_code = tl.load(state2_code_ptr + secondary_idx, mask=mask)\n    scale1 = primary_absmax / primary_code\n    scale2 = secondary_absmax / secondary_code\n    result = (tl.cast(q_val, tl.float32) - primary_offset) * scale1 * scale2\n\n    tl.store(output_ptr + offsets, tl.cast(result, tl.float16), mask=mask)\n\n@jit\ndef _your_dequantize_nf4_kernel_asm(\n    weight_ptr, \n    quant_absmax_ptr, \n    quant_code_ptr, \n    quant_offset_ptr, \n    state2_absmax_ptr,\n    state2_code_ptr,\n    output_ptr,\n    evict_ptr,               # new pointer for cache eviction\n    N: tl.constexpr,         # total number of dequantized elements\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    # Compute output indices.\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Dummy cache eviction load.\n    _ = tl.load(evict_ptr + offsets, mask=mask, other=0)\n\n    # Each uint8 yields 2 nf4 values.\n    packed_indices = offsets // 2\n    \n    # Use the same approach as the vectorized kernel for consistency\n    vec_size = 4\n    vec_indices = packed_indices // vec_size  # index in the uint32 view.\n    rem = packed_indices % vec_size           # which byte in the 32-bit word.\n\n    # Load 32 bits (i.e. 4 uint8) at once.\n    vec_data = tl.load(weight_ptr + vec_indices, mask=mask, other=0)\n    # Extract the desired byte.\n    byte_val = (vec_data >> (rem * 8)) & 0xFF\n\n    # Compute nibble selector: 0 for lower nibble, 1 for upper nibble.\n    nibble_selector = offsets % 2\n    lower_nibble = byte_val & 0xF\n    upper_nibble = byte_val >> 4\n    q_val = tl.where(nibble_selector == 0, lower_nibble, upper_nibble)\n\n    # Load quantization parameters.\n    primary_idx = offsets // 64\n    secondary_idx = offsets // 256\n    primary_absmax = tl.cast(tl.load(quant_absmax_ptr + primary_idx, mask=mask), tl.float32)\n    primary_code = tl.load(quant_code_ptr + primary_idx, mask=mask)\n    primary_offset = tl.load(quant_offset_ptr + primary_idx, mask=mask)\n    secondary_absmax = tl.load(state2_absmax_ptr + secondary_idx, mask=mask)\n    secondary_code = tl.load(state2_code_ptr + secondary_idx, mask=mask)\n    scale1 = primary_absmax / primary_code\n    scale2 = secondary_absmax / secondary_code\n    result = (tl.cast(q_val, tl.float32) - primary_offset) * scale1 * scale2\n\n    tl.store(output_ptr + offsets, tl.cast(result, tl.float16), mask=mask)\n\n# Alternative optimized ASM implementation that should be faster\n@jit\ndef _your_optimized_dequantize_nf4_kernel_asm(\n    weight_ptr, \n    quant_absmax_ptr, \n    quant_code_ptr, \n    quant_offset_ptr, \n    state2_absmax_ptr,\n    state2_code_ptr,\n    output_ptr,\n    evict_ptr,               # new pointer for cache eviction\n    N: tl.constexpr,         # total number of dequantized elements\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    # Compute output indices.\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Dummy cache eviction load.\n    _ = tl.load(evict_ptr + offsets, mask=mask, other=0)\n\n    # Each uint8 yields 2 nf4 values.\n    packed_indices = offsets // 2  \n    \n    # Load bytes directly\n    byte_val = tl.load(weight_ptr + packed_indices, mask=mask, other=0)\n\n    # Compute nibble selector: 0 for lower nibble, 1 for upper nibble.\n    nibble_selector = offsets % 2\n    lower_nibble = byte_val & 0xF\n    upper_nibble = byte_val >> 4\n    q_val = tl.where(nibble_selector == 0, lower_nibble, upper_nibble)\n\n    # Load quantization parameters.\n    primary_idx = offsets // 64\n    secondary_idx = offsets // 256\n    primary_absmax = tl.cast(tl.load(quant_absmax_ptr + primary_idx, mask=mask), tl.float32)\n    primary_code = tl.load(quant_code_ptr + primary_idx, mask=mask)\n    primary_offset = tl.load(quant_offset_ptr + primary_idx, mask=mask)\n    secondary_absmax = tl.load(state2_absmax_ptr + secondary_idx, mask=mask)\n    secondary_code = tl.load(state2_code_ptr + secondary_idx, mask=mask)\n    \n    # Fuse the scales for better performance\n    fused_scale = (primary_absmax / primary_code) * (secondary_absmax / secondary_code)\n    result = (tl.cast(q_val, tl.float32) - primary_offset) * fused_scale\n\n    tl.store(output_ptr + offsets, tl.cast(result, tl.float16), mask=mask)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T15:13:07.720612Z","iopub.execute_input":"2025-03-17T15:13:07.720909Z","iopub.status.idle":"2025-03-17T15:13:07.742436Z","shell.execute_reply.started":"2025-03-17T15:13:07.720883Z","shell.execute_reply":"2025-03-17T15:13:07.741460Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## host side dequantisation","metadata":{}},{"cell_type":"code","source":"##################################\n# HOST-SIDE DEQUANTIZATION FUNC.\n##################################\n\ndef _your_dequantize_nf4(weight, quant_state, use_custom_asm=False, use_cache_eviction=False, use_optimized=False):\n    N = weight.numel() * 2  # each uint8 yields 2 nf4 values.\n    output = torch.empty(N, dtype=torch.float16, device=weight.device)\n    # Get quantization parameter tensors.\n    quant_absmax = quant_state.absmax.contiguous()\n    quant_code = quant_state.code.contiguous()\n    quant_offset = quant_state.offset.contiguous()\n    state2_absmax = quant_state.state2.absmax.contiguous()\n    state2_code = quant_state.state2.code.contiguous()\n    BLOCK_SIZE = 4096\n    grid = lambda meta: (cdiv(N, meta['BLOCK_SIZE']),)\n\n    # Allocate an eviction buffer if desired.\n    if use_cache_eviction:\n        # Allocate a buffer of size BLOCK_SIZE (uint8); this can be tuned.\n        evict = torch.empty(BLOCK_SIZE, dtype=torch.uint8, device=weight.device)\n    else:\n        evict = torch.empty(1, dtype=torch.uint8, device=weight.device)\n\n    if use_custom_asm:\n        if use_optimized:\n            _your_optimized_dequantize_nf4_kernel_asm[grid](\n                weight, \n                quant_absmax, \n                quant_code, \n                quant_offset,\n                state2_absmax, \n                state2_code, \n                output,\n                evict,\n                N,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n        else:\n            _your_dequantize_nf4_kernel_asm[grid](\n                weight, \n                quant_absmax, \n                quant_code, \n                quant_offset,\n                state2_absmax, \n                state2_code, \n                output,\n                evict,\n                N,\n                BLOCK_SIZE=BLOCK_SIZE\n            )\n    else:\n        _your_dequantize_nf4_kernel_vectorized[grid](\n            weight, \n            quant_absmax, \n            quant_code, \n            quant_offset,\n            state2_absmax, \n            state2_code, \n            output,\n            evict,\n            N,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n    torch.cuda.synchronize()\n    return output\n\ndef your_dequantize_nf4(weight_obj, use_custom_asm=False, use_cache_eviction=False, use_optimized=False):\n    deq_flat = _your_dequantize_nf4(weight_obj.data, weight_obj.quant_state, use_custom_asm, use_cache_eviction, use_optimized)\n    if hasattr(weight_obj, \"data_shape\"):\n        num_elements = 1\n        for d in weight_obj.data_shape:\n            num_elements *= d\n        deq_reshaped = deq_flat[:num_elements].reshape(weight_obj.data_shape)\n    else:\n        deq_reshaped = deq_flat\n    target_dtype = getattr(weight_obj.quant_state, \"dtype\", torch.float16)\n    if target_dtype != torch.float16:\n        deq_reshaped = deq_reshaped.to(target_dtype)\n    return deq_reshaped\n\n# New function that employs the custom ASM kernel.\ndef custom_asm_dequantize_nf4(weight_obj, use_cache_eviction=False):\n    return your_dequantize_nf4(weight_obj, use_custom_asm=True, use_cache_eviction=use_cache_eviction)\n\n# New function that uses optimized ASM implementation\ndef optimized_asm_dequantize_nf4(weight_obj, use_cache_eviction=False):\n    return your_dequantize_nf4(weight_obj, use_custom_asm=True, use_cache_eviction=use_cache_eviction, use_optimized=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T15:13:13.402326Z","iopub.execute_input":"2025-03-17T15:13:13.402677Z","iopub.status.idle":"2025-03-17T15:13:13.411269Z","shell.execute_reply.started":"2025-03-17T15:13:13.402650Z","shell.execute_reply":"2025-03-17T15:13:13.410419Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Dummy Modules for testing","metadata":{}},{"cell_type":"code","source":"#############################\n# DUMMY MODULES FOR TESTING\n#############################\n\nclass DummyLinear4bit(nn.Module):\n    def __init__(self, in_features, out_features, dtype=torch.float16):\n        super().__init__()\n        self.data_shape = (out_features, in_features)\n        num_elements = out_features * in_features\n        num_packed = (num_elements + 1) // 2\n        self.quantized_weight = torch.randint(0, 255, (num_packed,), dtype=torch.uint8, device=\"cuda\")\n        num_dequantized = num_packed * 2\n        num_blocks1 = (num_dequantized + 63) // 64\n        self.quant_absmax = torch.randint(1, 10, (num_blocks1,), dtype=torch.uint8, device=\"cuda\")\n        self.quant_code = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n        self.quant_offset = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1\n        num_blocks2 = (num_dequantized + 255) // 256\n        state2_absmax = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.5 + 0.5\n        state2_code = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n        self.quant_state = type(\"QuantState\", (), {})()\n        self.quant_state.absmax = self.quant_absmax\n        self.quant_state.code = self.quant_code\n        self.quant_state.offset = self.quant_offset\n        self.quant_state.blocksize = 64\n        self.quant_state.state2 = type(\"State2\", (), {})()\n        self.quant_state.state2.absmax = state2_absmax\n        self.quant_state.state2.code = state2_code\n        self.quant_state.state2.blocksize = 256\n        self.quant_state.dtype = dtype\n        self.weight = type(\"WeightWrapper\", (), {})()\n        self.weight.data = self.quantized_weight\n        self.weight.quant_state = self.quant_state\n        self.weight.data_shape = self.data_shape\n        self.compute_dtype = dtype\n        self.use_custom_asm = False\n        self.use_optimized = False\n        \n    def forward(self, x):\n        if self.use_custom_asm:\n            if self.use_optimized:\n                dequant_weight = optimized_asm_dequantize_nf4(self.weight, use_cache_eviction=True)\n            else:\n                dequant_weight = custom_asm_dequantize_nf4(self.weight, use_cache_eviction=True)\n        else:\n            dequant_weight = your_dequantize_nf4(self.weight)\n        return x @ dequant_weight.t()\n    \n    def enable_custom_asm(self, enable=True, use_optimized=False):\n        self.use_custom_asm = enable\n        self.use_optimized = use_optimized\n        return self\n\ndef bnb_Linear4bit(in_features, out_features, dtype=torch.float16):\n    return DummyLinear4bit(in_features, out_features, dtype)\n\nclass MLP(nn.Module):\n    def __init__(self, hd=4096, m=14336, dtype=torch.float16):\n        super().__init__()\n        self.gate_proj = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n        self.up_proj   = bnb_Linear4bit(hd, m, dtype=dtype).to(\"cuda\")\n        self.down_proj = bnb_Linear4bit(m, hd, dtype=dtype).to(\"cuda\")\n        self.gate_proj.weight.quant_state.dtype = dtype\n        self.up_proj.weight.quant_state.dtype = dtype\n        self.down_proj.weight.quant_state.dtype = dtype\n        self.act_fn = F.silu\n        self.use_custom_asm = False\n        self.use_optimized = False\n        \n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n    \n    def enable_custom_asm(self, enable=True, use_optimized=False):\n        self.use_custom_asm = enable\n        self.use_optimized = use_optimized\n        self.gate_proj.enable_custom_asm(enable, use_optimized)\n        self.up_proj.enable_custom_asm(enable, use_optimized)\n        self.down_proj.enable_custom_asm(enable, use_optimized)\n        return self\n\ndef mlp_forward(X, mlp, dequantize_fx):\n    up   = X @ dequantize_fx(mlp.up_proj.weight).t()\n    gate = X @ dequantize_fx(mlp.gate_proj.weight).t()\n    h = mlp.act_fn(gate) * up\n    down = h @ dequantize_fx(mlp.down_proj.weight).t()\n    return down\n\ndef mlp_dequantize(X, mlp, dequantize_fx):\n    a = dequantize_fx(mlp.up_proj.weight).t(); torch.cuda.synchronize()\n    b = dequantize_fx(mlp.gate_proj.weight).t(); torch.cuda.synchronize()\n    c = dequantize_fx(mlp.down_proj.weight).t(); torch.cuda.synchronize()\n    return a, b, c\n\ndef unsloth_dequantize(weight_obj):\n    return your_dequantize_nf4(weight_obj)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T15:13:18.045447Z","iopub.execute_input":"2025-03-17T15:13:18.045742Z","iopub.status.idle":"2025-03-17T15:13:18.059629Z","shell.execute_reply.started":"2025-03-17T15:13:18.045718Z","shell.execute_reply":"2025-03-17T15:13:18.058804Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Test benchmarks and numerical evaluation","metadata":{}},{"cell_type":"code","source":"#####################################\n# TEST BENCHMARK & NUMERICAL VALIDATION\n#####################################\n\ndef test_dequantize(dequantize_fx, name=\"Your implementation\"):\n    elapsed = 0\n    results = []\n    options = [\n        (2, 3333, 2048, 8192, 3407, torch.float16),\n        (5, 777, 1024, 4096, 3409, torch.bfloat16),\n        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n    ]\n    \n    print(f\"\\n==== Testing {name} ====\")\n    for i, (bsz, qlen, hd, m, seed, dt) in enumerate(options):\n        torch.manual_seed(seed)\n        torch.set_default_dtype(torch.float32)\n        mlp = MLP(hd=hd, m=m, dtype=dt).to(\"cuda\")\n        X = torch.randn((bsz, qlen, hd), device=\"cuda\", dtype=dt) * 0.01\n        \n        # Test configuration details\n        config_name = f\"Config {i+1}: batch={bsz}, seq_len={qlen}, hidden={hd}, ffn={m}, dtype={dt}\"\n        print(f\"\\nTesting {config_name}\")\n        \n        torch.cuda.synchronize()\n        for _ in range(2):\n            out1 = mlp_forward(X, mlp, your_dequantize_nf4)\n            out2 = mlp(X)\n            assert torch.allclose(out1, out2, atol=1e-1), \\\n                \"Mismatch in forward outputs: max diff = \" + str((out1 - out2).abs().max().item())\n            a, b, c = mlp_dequantize(X, mlp, your_dequantize_nf4)\n            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n            assert torch.allclose(a, A, atol=1e-1), \\\n                \"Mismatch in dequantized up_proj: max diff = \" + str((a - A).abs().max().item())\n            assert torch.allclose(b, B, atol=1e-1), \\\n                \"Mismatch in dequantized gate_proj: max diff = \" + str((b - B).abs().max().item())\n            assert torch.allclose(c, C, atol=1e-1), \\\n                \"Mismatch in dequantized down_proj: max diff = \" + str((c - C).abs().max().item())\n        \n        torch.cuda.synchronize()\n        start = time.time()\n        num_iterations = 1000\n        for _ in range(num_iterations):\n            mlp_dequantize(X, mlp, dequantize_fx)\n        torch.cuda.synchronize()\n        \n        config_time = time.time() - start\n        elapsed += config_time\n        \n        total_weight_elements = 2 * (mlp.up_proj.weight.data_shape[0] * mlp.up_proj.weight.data_shape[1] + \n                                    mlp.gate_proj.weight.data_shape[0] * mlp.gate_proj.weight.data_shape[1] + \n                                    mlp.down_proj.weight.data_shape[0] * mlp.down_proj.weight.data_shape[1])\n        ops_per_second = (total_weight_elements * num_iterations) / config_time / 1e9  # in billions\n        \n        results.append({\n            \"config\": config_name,\n            \"time\": config_time,\n            \"iterations\": num_iterations,\n            \"ops_per_second\": ops_per_second,\n            \"weight_elements\": total_weight_elements\n        })\n        \n        print(f\"  Time: {config_time:.4f} seconds for {num_iterations} iterations\")\n        print(f\"  Speed: {ops_per_second:.2f} billion elements/second\")\n        \n    print(f\"\\nTotal elapsed time for {name}: {elapsed:.4f} seconds\")\n    return elapsed, results\n\ndef benchmark_and_compare():\n    print(\"\\n=== STARTING BENCHMARK AND COMPARISON ===\\n\")\n    \n    your_time, your_results = test_dequantize(your_dequantize_nf4, \"Base implementation\")\n    custom_asm_time, custom_asm_results = test_dequantize(custom_asm_dequantize_nf4, \"Fixed ASM implementation\")\n    optimized_asm_time, optimized_asm_results = test_dequantize(optimized_asm_dequantize_nf4, \"Optimized ASM implementation\")\n    reference_time, ref_results = test_dequantize(unsloth_dequantize, \"Reference implementation\")\n    \n    base_speedup = reference_time / your_time\n    fixed_asm_speedup = reference_time / custom_asm_time\n    # We ignore optimized ASM for the summary as before.\n    fixed_vs_base_speedup = your_time / custom_asm_time\n    \n    print(\"\\n=== BENCHMARK RESULTS ===\")\n    print(f\"Base implementation total time: {your_time:.4f} seconds\")\n    print(f\"Fixed ASM implementation total time: {custom_asm_time:.4f} seconds\")\n    print(f\"Optimized ASM implementation total time: {optimized_asm_time:.4f} seconds\")\n    print(f\"Reference implementation total time: {reference_time:.4f} seconds\")\n    print(f\"BASE SPEEDUP: {base_speedup:.2f}x (reference_time / base_time)\")\n    print(f\"FIXED ASM SPEEDUP: {fixed_asm_speedup:.2f}x (reference_time / fixed_asm_time)\")\n    print(f\"FIXED ASM vs BASE SPEEDUP: {fixed_vs_base_speedup:.2f}x (base_time / fixed_asm_time)\")\n    \n    print(\"\\n=== DETAILED CONFIGURATION COMPARISON ===\")\n    for i in range(len(your_results)):\n        your_config = your_results[i]\n        fixed_asm_config = custom_asm_results[i]\n        optimized_asm_config = optimized_asm_results[i]\n        ref_config = ref_results[i]\n        \n        base_config_speedup = ref_config[\"time\"] / your_config[\"time\"]\n        fixed_asm_config_speedup = ref_config[\"time\"] / fixed_asm_config[\"time\"]\n        optimized_asm_config_speedup = ref_config[\"time\"] / optimized_asm_config[\"time\"]\n        fixed_vs_base_config_speedup = your_config[\"time\"] / fixed_asm_config[\"time\"]\n        optimized_vs_base_config_speedup = your_config[\"time\"] / optimized_asm_config[\"time\"]\n        \n        print(f\"\\n{your_config['config']}\")\n        print(f\"  Base implementation: {your_config['time']:.4f} seconds, {your_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Fixed ASM: {fixed_asm_config['time']:.4f} seconds, {fixed_asm_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Optimized ASM: {optimized_asm_config['time']:.4f} seconds, {optimized_asm_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Reference implementation: {ref_config['time']:.4f} seconds, {ref_config['ops_per_second']:.2f} B elements/s\")\n        print(f\"  Base vs Reference speedup: {base_config_speedup:.2f}x\")\n        print(f\"  Fixed ASM vs Reference speedup: {fixed_asm_config_speedup:.2f}x\")\n        print(f\"  Optimized ASM vs Reference speedup: {optimized_asm_config_speedup:.2f}x\")\n        print(f\"  Fixed ASM vs Base speedup: {fixed_vs_base_config_speedup:.2f}x\")\n        print(f\"  Optimized ASM vs Base speedup: {optimized_vs_base_config_speedup:.2f}x\")\n    \n    return base_speedup, fixed_asm_speedup, fixed_vs_base_speedup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T15:20:43.948103Z","iopub.execute_input":"2025-03-17T15:20:43.948415Z","iopub.status.idle":"2025-03-17T15:20:43.971307Z","shell.execute_reply.started":"2025-03-17T15:20:43.948391Z","shell.execute_reply":"2025-03-17T15:20:43.970447Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Main calls ","metadata":{}},{"cell_type":"code","source":"#####################################\n# MAIN TESTING & BENCHMARKING ENTRY\n#####################################\n\nif __name__ == '__main__':\n    dummy_weight = torch.randint(0, 255, (1024,), dtype=torch.uint8, device=\"cuda\")\n    dummy_quant_state = type(\"DummyQuantState\", (), {})()\n    num_elements = 1024\n    num_packed = (num_elements + 1) // 2\n    num_dequantized = num_packed * 2\n    num_blocks1 = (num_dequantized + 63) // 64\n    dummy_quant_state.absmax = torch.randint(1, 10, (num_blocks1,), dtype=torch.uint8, device=\"cuda\")\n    dummy_quant_state.code = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n    dummy_quant_state.offset = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1\n    dummy_quant_state.blocksize = 64\n    num_blocks2 = (num_dequantized + 255) // 256\n    state2 = type(\"DummyState2\", (), {})()\n    state2.absmax = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.5 + 0.5\n    state2.code = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9\n    state2.blocksize = 256\n    dummy_quant_state.state2 = state2\n    \n    class DummyWeight:\n        def __init__(self, weight, quant_state, shape):\n            self.data = weight\n            self.quant_state = quant_state\n            self.data_shape = shape\n    \n    dummy_obj = DummyWeight(dummy_weight, dummy_quant_state, (num_elements,))\n    \n    print(\"Testing your_dequantize_nf4 directly:\")\n    out = your_dequantize_nf4(dummy_obj)\n    print(\"Direct kernel output sample (first 10 elements):\", out.view(-1)[:10])\n    \n    print(\"\\nTesting fixed custom_asm_dequantize_nf4 directly:\")\n    out_asm = custom_asm_dequantize_nf4(dummy_obj, use_cache_eviction=True)\n    print(\"Fixed ASM kernel output sample (first 10 elements):\", out_asm.view(-1)[:10])\n    \n    print(\"\\nTesting optimized_asm_dequantize_nf4 directly:\")\n    out_optimized = optimized_asm_dequantize_nf4(dummy_obj, use_cache_eviction=True)\n    print(\"Optimized ASM kernel output sample (first 10 elements):\", out_optimized.view(-1)[:10])\n    \n    print(\"\\nChecking numerical consistency between base and fixed ASM implementations:\")\n    max_diff = (out - out_asm).abs().max().item()\n    print(f\"Maximum difference between implementations: {max_diff}\")\n    if max_diff < 1e-1:\n        print(\"PASSED: Implementations are numerically consistent\")\n    else:\n        print(\"WARNING: Implementations show numerical differences\")\n    \n    base_speedup, asm_speedup, asm_vs_base_speedup = benchmark_and_compare()\n    \n    print(\"\\n=== SUMMARY ===\")\n    print(f\"Base vs Reference speedup ratio: {base_speedup:.2f}x\")\n    print(f\"Custom ASM vs Reference speedup ratio: {asm_speedup:.2f}x\")\n    print(f\"Custom ASM vs Base speedup ratio: {asm_vs_base_speedup:.2f}x\")\n    \n    if asm_speedup > base_speedup:\n        improvement = (asm_speedup - base_speedup) / base_speedup * 100\n        print(f\"The Custom ASM implementation is {improvement:.2f}% faster than the base implementation.\")\n    elif asm_speedup < base_speedup:\n        degradation = (base_speedup - asm_speedup) / base_speedup * 100\n        print(f\"The Custom ASM implementation is {degradation:.2f}% slower than the base implementation.\")\n    else:\n        print(\"The Custom ASM implementation has the SAME SPEED as the base implementation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T15:22:29.943900Z","iopub.execute_input":"2025-03-17T15:22:29.944182Z","iopub.status.idle":"2025-03-17T15:23:29.620286Z","shell.execute_reply.started":"2025-03-17T15:22:29.944160Z","shell.execute_reply":"2025-03-17T15:23:29.619459Z"}},"outputs":[{"name":"stdout","text":"Testing your_dequantize_nf4 directly:\nDirect kernel output sample (first 10 elements): tensor([58.0625, 36.8438, -0.3096, -0.3096, -0.3096, -0.3096, -0.3096, -0.3096,\n        36.8438, 20.9219], device='cuda:0', dtype=torch.float16)\n\nTesting fixed custom_asm_dequantize_nf4 directly:\nFixed ASM kernel output sample (first 10 elements): tensor([58.0625, 36.8438, -0.3096, -0.3096, -0.3096, -0.3096, -0.3096, -0.3096,\n        36.8438, 20.9219], device='cuda:0', dtype=torch.float16)\n\nTesting optimized_asm_dequantize_nf4 directly:\nOptimized ASM kernel output sample (first 10 elements): tensor([58.0625, 36.8438, 36.8438, 20.9219, 47.4688, 74.0000, 52.7812, 68.6875,\n        79.3125, 47.4688], device='cuda:0', dtype=torch.float16)\n\nChecking numerical consistency between base and fixed ASM implementations:\nMaximum difference between implementations: 0.0\nPASSED: Implementations are numerically consistent\n\n=== STARTING BENCHMARK AND COMPARISON ===\n\n\n==== Testing Base implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 0.9626 seconds for 1000 iterations\n  Speed: 104.58 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 0.8479 seconds for 1000 iterations\n  Speed: 29.68 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 7.5443 seconds for 1000 iterations\n  Speed: 46.70 billion elements/second\n\nTotal elapsed time for Base implementation: 9.3547 seconds\n\n==== Testing Fixed ASM implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 0.9618 seconds for 1000 iterations\n  Speed: 104.66 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 0.8658 seconds for 1000 iterations\n  Speed: 29.07 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 7.8627 seconds for 1000 iterations\n  Speed: 44.81 billion elements/second\n\nTotal elapsed time for Fixed ASM implementation: 9.6904 seconds\n\n==== Testing Optimized ASM implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 1.1115 seconds for 1000 iterations\n  Speed: 90.57 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 0.8776 seconds for 1000 iterations\n  Speed: 28.68 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 9.1069 seconds for 1000 iterations\n  Speed: 38.69 billion elements/second\n\nTotal elapsed time for Optimized ASM implementation: 11.0960 seconds\n\n==== Testing Reference implementation ====\n\nTesting Config 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Time: 0.9960 seconds for 1000 iterations\n  Speed: 101.06 billion elements/second\n\nTesting Config 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Time: 0.8462 seconds for 1000 iterations\n  Speed: 29.74 billion elements/second\n\nTesting Config 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Time: 7.7443 seconds for 1000 iterations\n  Speed: 45.49 billion elements/second\n\nTotal elapsed time for Reference implementation: 9.5865 seconds\n\n=== BENCHMARK RESULTS ===\nBase implementation total time: 9.3547 seconds\nFixed ASM implementation total time: 9.6904 seconds\nOptimized ASM implementation total time: 11.0960 seconds\nReference implementation total time: 9.5865 seconds\nBASE SPEEDUP: 1.02x (reference_time / base_time)\nFIXED ASM SPEEDUP: 0.99x (reference_time / fixed_asm_time)\nFIXED ASM vs BASE SPEEDUP: 0.97x (base_time / fixed_asm_time)\n\n=== DETAILED CONFIGURATION COMPARISON ===\n\nConfig 1: batch=2, seq_len=3333, hidden=2048, ffn=8192, dtype=torch.float16\n  Base implementation: 0.9626 seconds, 104.58 B elements/s\n  Fixed ASM: 0.9618 seconds, 104.66 B elements/s\n  Optimized ASM: 1.1115 seconds, 90.57 B elements/s\n  Reference implementation: 0.9960 seconds, 101.06 B elements/s\n  Base vs Reference speedup: 1.03x\n  Fixed ASM vs Reference speedup: 1.04x\n  Optimized ASM vs Reference speedup: 0.90x\n  Fixed ASM vs Base speedup: 1.00x\n  Optimized ASM vs Base speedup: 0.87x\n\nConfig 2: batch=5, seq_len=777, hidden=1024, ffn=4096, dtype=torch.bfloat16\n  Base implementation: 0.8479 seconds, 29.68 B elements/s\n  Fixed ASM: 0.8658 seconds, 29.07 B elements/s\n  Optimized ASM: 0.8776 seconds, 28.68 B elements/s\n  Reference implementation: 0.8462 seconds, 29.74 B elements/s\n  Base vs Reference speedup: 1.00x\n  Fixed ASM vs Reference speedup: 0.98x\n  Optimized ASM vs Reference speedup: 0.96x\n  Fixed ASM vs Base speedup: 0.98x\n  Optimized ASM vs Base speedup: 0.97x\n\nConfig 3: batch=3, seq_len=2048, hidden=4096, ffn=14336, dtype=torch.bfloat16\n  Base implementation: 7.5443 seconds, 46.70 B elements/s\n  Fixed ASM: 7.8627 seconds, 44.81 B elements/s\n  Optimized ASM: 9.1069 seconds, 38.69 B elements/s\n  Reference implementation: 7.7443 seconds, 45.49 B elements/s\n  Base vs Reference speedup: 1.03x\n  Fixed ASM vs Reference speedup: 0.98x\n  Optimized ASM vs Reference speedup: 0.85x\n  Fixed ASM vs Base speedup: 0.96x\n  Optimized ASM vs Base speedup: 0.83x\n\n=== SUMMARY ===\nBase vs Reference speedup ratio: 1.02x\nCustom ASM vs Reference speedup ratio: 0.99x\nCustom ASM vs Base speedup ratio: 0.97x\nThe Custom ASM implementation is 3.46% slower than the base implementation.\n","output_type":"stream"}],"execution_count":28}]}