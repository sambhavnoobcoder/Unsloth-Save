{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UNSLOTH CHALLENGE 1 SUBMISSION : Convert nf4 to Triton.","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"NF4\"></a>\n## A) Convert `nf4` to Triton. [Difficulty: Hard] [Max points: 14]\n\n1. Goal: Convert a `nf4` quantized tensor into `fp16` or `bf16` into a *single* Triton kernel The double dequant of the `absmax` and weight forming must be done in 1 Triton kernel. Must work on Tesla T4.\n2. Must be faster than Unsloth's `fast_dequantize` by 1.15x or more, and not use large intermediate memory buffers.\n3. Must not use `torch.compile`, but can use `trace.enabled` to help on writing Triton kernels.\n4. Good material: [Unsloth `fast_dequantize` function](https://github.com/unslothai/unsloth/blob/main/unsloth/kernels/utils.py#L128), also [bitsandbytes `dequantize_blockwise`](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/86b6c37a8ad448230cedb60753f63150b603a112/bitsandbytes/functional.py#L958)\n5. Use `test_dequantize_function` to test your implementation.\n6. No CUDA allowed. Custom CUDA inside of the Triton is allowed.\n7. Watch Tim's videos on Youtube: [8-bit Optimizers](https://www.youtube.com/watch?v=2ETNONas068)","metadata":{}},{"cell_type":"markdown","source":"## Evaluation parameters ","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for A) Max points = 14\n```python\nif attemped_A:\n    A_score = 0\n    if single_triton_kernel: A_score += 3\n    speedup = old_time / new_time\n    if speedup <= 1.00: A_score -= 3\n    if speedup >= 1.05: A_score += 1\n    if speedup >= 1.10: A_score += 2\n    if speedup >= 1.15: A_score += 2\n    if kernel_works_in_torch_compile: A_score += 1\n    else: A_score -= 1\n    if custom_asm_works: A_score += 3\n    if uses_cache_eviction: A_score += 1\n    if tested_in_f16_and_bf16: A_score += 1\n    else: A_score -= 1\n    final_score += A_score\nelse:\n    final_score += 0\n```","metadata":{}},{"cell_type":"markdown","source":"lets load up the basic libraries","metadata":{}},{"cell_type":"code","source":"!pip install triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:32:48.551143Z","iopub.execute_input":"2025-03-17T14:32:48.551448Z","iopub.status.idle":"2025-03-17T14:33:01.735653Z","shell.execute_reply.started":"2025-03-17T14:32:48.551419Z","shell.execute_reply":"2025-03-17T14:33:01.734596Z"}},"outputs":[{"name":"stdout","text":"Collecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"phase one - get it to compile","metadata":{}},{"cell_type":"code","source":"import torch\nfrom triton import jit, cdiv\nimport triton.language as tl\n\n# Kernel: Traverses the input tensor in blocks and copies each element after a simple cast.\n@jit\ndef _your_dequantize_nf4_kernel(weight_ptr, quant_state_ptr, output_ptr, num_elements: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n    # Get the unique program (block) ID.\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    # Create a vector of indices for the block.\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to avoid out-of-bound accesses.\n    mask = offsets < num_elements\n\n    # Load values from the input weight tensor.\n    # (Note: Currently, we ignore quant_state in the arithmetic.)\n    values = tl.load(weight_ptr + offsets, mask=mask)\n    # For now, simply cast the values to float16 and store in output.\n    tl.store(output_ptr + offsets, tl.cast(values, tl.float16), mask=mask)\n\ndef _your_dequantize_nf4(weight, quant_state):\n    # Debug prints on the host side\n    print(\"=== Starting _your_dequantize_nf4 ===\")\n    print(\"Weight tensor shape:\", weight.shape)\n    print(\"Weight tensor dtype:\", weight.dtype)\n    try:\n        print(\"Quant state shape:\", quant_state.shape)\n    except AttributeError:\n        print(\"Quant state does not have a shape attribute (likely not a tensor).\")\n    \n    # Total number of elements in the weight tensor.\n    num_elements = weight.numel()\n    print(\"Total number of elements:\", num_elements)\n    \n    # Allocate an output tensor on the same device, with fp16 precision.\n    output = torch.empty(num_elements, dtype=torch.float16, device=weight.device)\n    \n    # Determine grid size based on BLOCK_SIZE.\n    grid = lambda meta: (cdiv(num_elements, meta['BLOCK_SIZE']),)\n    \n    # Launch the kernel.\n    _your_dequantize_nf4_kernel[grid](weight, quant_state, output, num_elements, BLOCK_SIZE=1024)\n    \n    # Synchronize and print a small sample of the output for debugging.\n    torch.cuda.synchronize()\n    print(\"Kernel execution complete. Output sample (first 10 elements):\", output[:10])\n    print(\"=== Finished _your_dequantize_nf4 ===\")\n    return output\n\ndef your_dequantize_nf4(weight):\n    # weight is expected to be an object with attributes 'weight.data' and 'weight.quant_state'\n    print(\">>> Entering your_dequantize_nf4\")\n    output = _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)\n    print(\">>> Exiting your_dequantize_nf4\")\n    return output\n\n# For debugging purposes, you can create dummy inputs as follows:\nif __name__ == '__main__':\n    # Create a dummy quantized weight tensor (simulate with uint8 data).\n    dummy_weight = torch.randint(0, 255, (1024,), dtype=torch.uint8, device=\"cuda\")\n    # Create a dummy quant_state tensor (its content is not used in this phase).\n    dummy_quant_state = torch.empty((1,), dtype=torch.uint8, device=\"cuda\")\n    \n    # Wrap dummy_weight in an object with the expected attributes.\n    class DummyWeight:\n        def __init__(self, weight, quant_state):\n            self.weight = type(\"W\", (), {\"data\": weight, \"quant_state\": quant_state})\n    \n    dummy_obj = DummyWeight(dummy_weight, dummy_quant_state)\n    \n    # Run our dequantization kernel skeleton.\n    output = your_dequantize_nf4(dummy_obj)\n    print(\"Final output (first 10 elements):\", output[:10])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"double dequantisation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom triton import jit, cdiv\nimport triton.language as tl\n\n@jit\ndef _your_dequantize_nf4_kernel(\n    weight_ptr, \n    quant_absmax_ptr, \n    quant_code_ptr, \n    quant_offset_ptr, \n    state2_absmax_ptr,\n    state2_code_ptr,\n    output_ptr,\n    N: tl.constexpr,  # Total number of dequantized elements (i.e. weight.numel() * 2)\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Calculate global indices for the dequantized elements\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Map dequantized index 'i' to the packed uint8 index and nibble index\n    packed_indices = offsets // 2       # Two dequantized values per packed byte\n    nibble_selector = offsets % 2         # 0 => lower nibble, 1 => upper nibble\n\n    # Load the packed byte\n    byte_val = tl.load(weight_ptr + packed_indices, mask=mask)\n\n    # Unpack the nibble: lower nibble if selector==0, else upper nibble\n    lower = byte_val & 0xF\n    higher = byte_val >> 4\n    q_val = tl.where(nibble_selector == 0, lower, higher)\n\n    # Determine block indices for the two sets of quantization parameters\n    block1 = offsets // 64   # For quant_state.absmax, code, offset (block size = 64)\n    block2 = offsets // 256  # For state2.absmax and state2.code (block size = 256)\n\n    # Load block parameters for each dequantized element\n    amax = tl.load(quant_absmax_ptr + block1, mask=mask)\n    code_val = tl.load(quant_code_ptr + block1, mask=mask)\n    offset_val = tl.load(quant_offset_ptr + block1, mask=mask)\n    s2_amax = tl.load(state2_absmax_ptr + block2, mask=mask)\n    s2_code = tl.load(state2_code_ptr + block2, mask=mask)\n\n    # Compute scales: first scale from quant_state and then from state2\n    scale1 = tl.cast(amax, tl.float32) / code_val  # Cast amax from uint8 to float32\n    scale2 = s2_amax / s2_code\n\n    # Perform dequantization: adjust the nibble by subtracting offset, then scale\n    result = (tl.cast(q_val, tl.float32) - offset_val) * scale1 * scale2\n\n    # Store the dequantized value as fp16 (or bf16 with minor modifications)\n    tl.store(output_ptr + offsets, tl.cast(result, tl.float16), mask=mask)\n\ndef _your_dequantize_nf4(weight, quant_state):\n    print(\"=== Starting _your_dequantize_nf4 ===\")\n    print(\"Weight tensor shape:\", weight.shape)\n    print(\"Weight tensor dtype:\", weight.dtype)\n    try:\n        print(\"Quant state absmax shape:\", quant_state.absmax.shape)\n    except AttributeError:\n        print(\"Quant state missing attributes!\")\n    \n    # Total dequantized elements: each uint8 holds 2 nf4 values\n    N = weight.numel() * 2\n    print(\"Total number of dequantized elements:\", N)\n    \n    # Allocate output tensor (dequantized values) on the same device\n    output = torch.empty(N, dtype=torch.float16, device=weight.device)\n    \n    # Extract quant_state arrays\n    quant_absmax = quant_state.absmax.contiguous()\n    quant_code = quant_state.code.contiguous()\n    quant_offset = quant_state.offset.contiguous()\n    state2_absmax = quant_state.state2.absmax.contiguous()\n    state2_code = quant_state.state2.code.contiguous()\n    \n    # Set up kernel parameters and grid configuration\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (cdiv(N, meta['BLOCK_SIZE']),)\n    \n    # Launch the Triton kernel\n    _your_dequantize_nf4_kernel[grid](\n        weight, \n        quant_absmax, \n        quant_code, \n        quant_offset, \n        state2_absmax,\n        state2_code,\n        output, \n        N, \n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    torch.cuda.synchronize()\n    print(\"Kernel execution complete. Output sample (first 10 elements):\", output[:10])\n    print(\"=== Finished _your_dequantize_nf4 ===\")\n    return output\n\ndef your_dequantize_nf4(weight):\n    print(\">>> Entering your_dequantize_nf4\")\n    output = _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)\n    print(\">>> Exiting your_dequantize_nf4\")\n    return output\n\n# ----------------- Debug/Test Harness -----------------\n\nif __name__ == '__main__':\n    # Create a dummy quantized weight tensor with 1024 uint8 elements (packed nf4 data)\n    dummy_weight = torch.randint(0, 255, (1024,), dtype=torch.uint8, device=\"cuda\")\n    \n    # Total dequantized elements = 1024 * 2 = 2048\n    num_dequantized = dummy_weight.numel() * 2\n    # Calculate number of blocks for the first and second quantization states\n    num_blocks1 = (num_dequantized + 63) // 64   # block size = 64\n    num_blocks2 = (num_dequantized + 255) // 256   # block size = 256\n    \n    # Create dummy parameters (with simple values for testing)\n    dummy_quant_absmax = torch.randint(1, 255, (num_blocks1,), dtype=torch.uint8, device=\"cuda\")\n    dummy_quant_code = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9  # roughly near 1.0\n    dummy_quant_offset = torch.rand(num_blocks1, dtype=torch.float32, device=\"cuda\") * 0.1  # small offsets\n    \n    dummy_state2_absmax = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 2.0 + 0.5  # some scale\n    dummy_state2_code = torch.rand(num_blocks2, dtype=torch.float32, device=\"cuda\") * 0.1 + 0.9     # roughly near 1.0\n\n    # Create a dummy state2 object with required attributes\n    class DummyState2:\n        pass\n    dummy_state2 = DummyState2()\n    dummy_state2.absmax = dummy_state2_absmax\n    dummy_state2.code = dummy_state2_code\n    dummy_state2.blocksize = 256\n\n    # Create a dummy quant_state object with the required attributes and nested state2\n    class DummyQuantState:\n        pass\n    dummy_quant_state = DummyQuantState()\n    dummy_quant_state.absmax = dummy_quant_absmax\n    dummy_quant_state.code = dummy_quant_code\n    dummy_quant_state.offset = dummy_quant_offset\n    dummy_quant_state.blocksize = 64\n    dummy_quant_state.state2 = dummy_state2\n\n    # Wrap dummy_weight in an object that mimics the expected structure\n    class DummyWeight:\n        def __init__(self, weight, quant_state):\n            self.weight = type(\"W\", (), {\"data\": weight, \"quant_state\": quant_state})\n    \n    dummy_obj = DummyWeight(dummy_weight, dummy_quant_state)\n    \n    # Run the dequantization kernel\n    output = your_dequantize_nf4(dummy_obj)\n    print(\"Final output (first 10 elements):\", output[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T14:34:52.544021Z","iopub.execute_input":"2025-03-17T14:34:52.544433Z","iopub.status.idle":"2025-03-17T14:34:52.956797Z","shell.execute_reply.started":"2025-03-17T14:34:52.544405Z","shell.execute_reply":"2025-03-17T14:34:52.956105Z"}},"outputs":[{"name":"stdout","text":">>> Entering your_dequantize_nf4\n=== Starting _your_dequantize_nf4 ===\nWeight tensor shape: torch.Size([1024])\nWeight tensor dtype: torch.uint8\nQuant state absmax shape: torch.Size([32])\nTotal number of dequantized elements: 2048\nKernel execution complete. Output sample (first 10 elements): tensor([ 1.7860e+03, -4.2456e-01,  2.7425e+02,  5.4900e+02,  6.8650e+02,\n         8.2400e+02,  6.8650e+02,  1.7860e+03,  9.6150e+02,  1.0990e+03],\n       device='cuda:0', dtype=torch.float16)\n=== Finished _your_dequantize_nf4 ===\n>>> Exiting your_dequantize_nf4\nFinal output (first 10 elements): tensor([ 1.7860e+03, -4.2456e-01,  2.7425e+02,  5.4900e+02,  6.8650e+02,\n         8.2400e+02,  6.8650e+02,  1.7860e+03,  9.6150e+02,  1.0990e+03],\n       device='cuda:0', dtype=torch.float16)\n","output_type":"stream"}],"execution_count":3}]}