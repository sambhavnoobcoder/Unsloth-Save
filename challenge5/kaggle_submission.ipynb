{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UNSLOTH CHALLENGE 5 SUBMISSION - Memory Efficient Backprop","metadata":{}},{"cell_type":"markdown","source":"## Problem statement","metadata":{}},{"cell_type":"markdown","source":"---\n---\n---\n<a name=\"MATH\"></a>\n## E) Memory Efficient Backprop [Difficulty: Medium to Hard] [Max points: 10]\n\nIn LLMs, the last layer is a projection matrix to calculate the probabilities of the next token, ie $\\sigma(XW)$. However, if the vocabulary size is very large, say 128K, then the materialization of the logits causes VRAM spikes.\n\nFor example, if the `bsz = 4, qlen = 4096, hd = 4096, vocab = 128K`, then the memory usage for the logits in bfloat16 would be 4GB. In the worst case, we might even need to upcast logits to float32, so 8GB is needed.\n\nIn Unsloth, we utilize [Apple's Cut Cross Entropy Loss](https://machinelearning.apple.com/research/cut-your-losses) to reduce VRAM usage, by allowing a Triton kernel to create the logits on the fly to calculate the cross entropy loss. But this does not generalize well to other functions.\n\nOur goal is to generalize this ultimately, but directly creating logits on the fly will be hard. Instead, let's take a slightly less complex approach. Let's first review some stuff. We first notice that during the normal case after forming the intermediate logits for 2 batches, we then do a gather function to aggregate the intermediate results into a single column:\n$$\n\\begin{align}\n\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times W &= \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\\\\nf \\bigg( \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\bigg) &= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n\\end{align}\n$$\n\nSo, if we can somehow skip the materialization of the intermediate logits, and just output the output of `f`, we can save a lot of VRAM!\n\nNotice during backpropagation we can use the chain rule:\n$$\n\\begin{align}\n\\frac{dL}{dX} &= \\frac{dL}{dy} \\frac{dy}{dX} ; \\frac{dL}{dW} = \\frac{dL}{dy} \\frac{dy}{dW} \\\\\n\\frac{dL}{dy} &= \\text{Downstream from backprop} \\\\\n\\frac{dy}{dX} &= W^T \\\\\n\\frac{dy}{dW} &= X^T \\\\\n\\frac{dL}{dX} &= \\frac{dL}{dy} W^T \\\\\n\\frac{dL}{dW} &= X^T \\frac{dL}{dy} \\\\\n\\end{align}\n$$\n\nIf we simply compute the intermediate tensors on the fly via batches, say we do batch 1, then batch 2, we can reduce VRAM usage from 4GB to 2GB!\n\n$$\n\\begin{align}\n\\frac{dL}{dX} &= \\begin{bmatrix} \\frac{dL_1}{dy_1} W^T \\\\ \\frac{dL_2}{dy_2} W^T \\end{bmatrix} \\\\\n\\frac{dL}{dW} &= \\bigg( X_1^T \\frac{dL_1}{dy_1} + X_2^T  \\frac{dL_2}{dy_2} \\bigg)\n\\end{align}\n$$\n\n1. Your goal is to write a `torch.autograd.Function` with a `forward` and `backward` pass showcasing this memory efficient implementation.\n\n2. You must NOT hard code the derivatives - move the transformation function from the logits / intermeditate tensors to a smaller tensor as a separate function which can allow `autograd` to pass through it.\n\n3. As a hint, look at `torch.checkpoint` at https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py. Also, don't forget about the upstream gradients! We need to multiply them to the current gradients!\n\n4. Make the Cross Entropy Loss work. You must show other functions working as well.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation parameters","metadata":{}},{"cell_type":"markdown","source":"## Marking Criteria for E) Max points = 10\n```python\nif attemped_E:\n    E_score = 0\n    if VRAM_50_percent_reduction: E_score += 2\n    if remove_float32_upcast: E_score = 0\n    if show_ce_loss_works: E_score += 1\n    if show_other_functions_work: E_score += 1\n    if hardcoded_gradients: E_score = 0\n    if allows_dynamic_chunk_sizes: E_score += 1\n    if llama_1B_training_loss_matches: E_score += 1\n    else: E_score = 0\n    if GRPO_memory_efficient_linear_works: E_score += 4\n    final_score += E_score\nelse:\n    final_score += 0\n```","metadata":{}},{"cell_type":"markdown","source":"lets start by loading up the libraries necessary for this ","metadata":{}},{"cell_type":"code","source":"!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T10:40:09.438811Z","iopub.execute_input":"2025-03-17T10:40:09.439107Z","iopub.status.idle":"2025-03-17T10:40:13.636871Z","shell.execute_reply.started":"2025-03-17T10:40:09.439082Z","shell.execute_reply":"2025-03-17T10:40:13.635929Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"all you need is torch !!!! and you are good to goooo......  :) :0","metadata":{}},{"cell_type":"markdown","source":"starting out with a basic functional backprop ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef transformation_function(batch, weight, labels):\n    \"\"\"\n    Compute the logits and then the cross entropy loss in sum-reduction mode.\n    Note: batch is expected to be (B, S, D) and labels (B, S).\n    \"\"\"\n    # Compute logits: shape (B, S, vocab)\n    x = F.linear(batch, weight).float()\n    loss_fct = nn.CrossEntropyLoss(reduction=\"sum\")\n    # Flatten so that loss_fct sees (B*S, vocab) and (B*S,)\n    loss = loss_fct(x.view(-1, x.size(-1)), labels.view(-1))\n    return loss\n\nclass MemoryEfficientLinear(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, weight, labels, forward_function, batch_chunk_size, seq_chunk_size):\n        \"\"\"\n        X: Tensor of shape (B, S, D)\n        weight: Projection weight of shape (vocab, D)\n        labels: Tensor of shape (B*S,) - will be reshaped to (B, S)\n        forward_function: a function that computes loss for a given chunk.\n        batch_chunk_size: how many examples in the batch to process at once.\n        seq_chunk_size: how many tokens (sequence length) to process per chunk.\n        \"\"\"\n        # Save for backward\n        ctx.save_for_backward(X, weight, labels)\n        ctx.batch_chunk_size = batch_chunk_size\n        ctx.seq_chunk_size = seq_chunk_size\n        ctx.forward_function = forward_function\n\n        total_loss = 0.0\n        total_tokens = 0\n\n        B, S, _ = X.shape\n        # Reshape labels into (B, S) for easier chunking\n        labels_reshaped = labels.view(B, S)\n        for i in range(0, B, batch_chunk_size):\n            X_batch = X[i:i+batch_chunk_size]\n            labels_batch = labels_reshaped[i:i+batch_chunk_size]\n            for j in range(0, S, seq_chunk_size):\n                X_chunk = X_batch[:, j:j+seq_chunk_size]\n                labels_chunk = labels_batch[:, j:j+seq_chunk_size]\n                # Compute chunk loss (using sum reduction)\n                chunk_loss = forward_function(X_chunk, weight, labels_chunk)\n                total_loss += chunk_loss\n                total_tokens += X_chunk.size(0) * X_chunk.size(1)\n\n        # Average the total loss over tokens.\n        if total_tokens == 0:\n            total_loss_tensor = torch.tensor(0.0, device=X.device)\n        else:\n            total_loss_tensor = total_loss / total_tokens\n\n        ctx.total_tokens = total_tokens\n        ctx.input_shape = X.shape\n        return total_loss_tensor\n\n    @staticmethod\n    def backward(ctx, d_loss):\n        X, W, labels = ctx.saved_tensors\n        batch_chunk_size = ctx.batch_chunk_size\n        seq_chunk_size = ctx.seq_chunk_size\n        forward_function = ctx.forward_function\n        total_tokens = ctx.total_tokens\n        B, S, _ = X.shape\n\n        # Allocate gradients with the same shape as the inputs\n        d_X = torch.zeros_like(X) if X.requires_grad else None\n        d_W = torch.zeros_like(W) if W.requires_grad else None\n\n        # Reshape labels to (B, S)\n        labels_reshaped = labels.view(B, S)\n\n        # Loop over both dimensions\n        for i in range(0, B, batch_chunk_size):\n            X_batch = X[i:i+batch_chunk_size]\n            labels_batch = labels_reshaped[i:i+batch_chunk_size]\n            for j in range(0, S, seq_chunk_size):\n                # Detach the chunk and set requires_grad for recomputation\n                X_chunk = X_batch[:, j:j+seq_chunk_size].detach().requires_grad_(True)\n                labels_chunk = labels_batch[:, j:j+seq_chunk_size]\n                with torch.enable_grad():\n                    # Recompute the loss for the chunk\n                    chunk_loss = forward_function(X_chunk, W, labels_chunk)\n                    # Scale the loss contribution as in the forward pass\n                    local_loss = chunk_loss / total_tokens\n                    # Compute gradients with respect to X_chunk and W.\n                    gX, gW = torch.autograd.grad(local_loss, (X_chunk, W), retain_graph=True)\n                if d_X is not None:\n                    d_X[i:i+batch_chunk_size, j:j+seq_chunk_size] += gX * d_loss\n                if d_W is not None:\n                    d_W += gW * d_loss\n\n        # Return gradients for all six inputs: (d_X, d_W, None, None, None, None)\n        return d_X, d_W, None, None, None, None\n\n# Example usage and test\nif __name__ == \"__main__\":\n    device = 'cuda'\n    # Test parameters as given:\n    bsz, qlen, hd, vocab = 4, 4096, 4096, 128000\n    X = torch.randn(bsz, qlen, hd, dtype=torch.bfloat16, device=device, requires_grad=True)\n    W = torch.randn(vocab, hd, dtype=torch.bfloat16, device=device, requires_grad=True)\n    labels = torch.randint(0, vocab, (bsz * qlen,), device=device)\n\n    # Call using positional arguments (do not use keyword arguments)\n    loss = MemoryEfficientLinear.apply(X, W, labels, transformation_function, 1, 1024)\n    loss.backward()\n\n    print(\"Loss:\", loss.item())\n    print(\"Gradients for X computed:\", X.grad is not None)\n    print(\"Gradients for W computed:\", W.grad is not None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T10:40:27.688652Z","iopub.execute_input":"2025-03-17T10:40:27.689093Z","iopub.status.idle":"2025-03-17T10:40:59.238846Z","shell.execute_reply.started":"2025-03-17T10:40:27.689053Z","shell.execute_reply":"2025-03-17T10:40:59.237928Z"}},"outputs":[{"name":"stdout","text":"Loss: 283.1170959472656\nGradients for X computed: True\nGradients for W computed: True\n","output_type":"stream"}],"execution_count":2}]}